{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"ParsBench","text":""},{"location":"#overview","title":"Overview","text":"<p>ParsBench provides toolkits for benchmarking Large Language Models (LLMs) based on the Persian language. It includes various tasks for evaluating LLMs on different topics, benchmarking tools to compare multiple models and rank them, and an easy, fully customizable API for developers to create custom models, tasks, scores, and benchmarks.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Variety of Tasks: Evaluate LLMs across various topics.</li> <li>Benchmarking Tools: Compare and rank multiple models.</li> <li>Customizable API: Create custom models, tasks, scores, and benchmarks with ease.</li> </ul>"},{"location":"#motivation","title":"Motivation","text":"<p>I was trying to fine-tune an open-source LLM for the Persian language. I needed some evaluation to test the performance and utility of my LLM. It leads me to research and find this paper. It's great work that they prepared some datasets and evaluation methods to test on ChatGPT. They even shared their code in this repository.</p> <p>So, I thought that I should build a handy framework that includes various tasks and datasets for evaluating LLMs based on the Persian language. I used some parts of their work (Datasets, Metrics, Basic prompt templates) in this library.</p>"},{"location":"#example-notebooks","title":"Example Notebooks","text":"<ul> <li>Benchmark Aya models: </li> <li>Benchmark Ava models: </li> <li>Benchmark Dorna models: </li> <li>Benchmark MaralGPT models: </li> </ul>"},{"location":"#sponsors","title":"Sponsors","text":"<p>Here are the names of companies/people who helped us to keep maintaining this project. If you want to donate this project, see this page.</p> <ul> <li>AvalAI: They gave us free OpenAI API credit several times in their \"AvalAward\" program. It helped us for doing R&amp;D and benchmarking GPT models.</li> <li>Basalam: They voluntarily helped us to run the benchmarks on open-weight models and build the ParsBench Leaderboard.</li> </ul>"},{"location":"#contributing","title":"Contributing","text":"<p>Contributions are welcome! Please refer to the contribution guidelines for more information on how to contribute.</p>"},{"location":"#license","title":"License","text":"<p>ParsBench is distributed under the Apache-2.0 license.</p>"},{"location":"#contact-information","title":"Contact Information","text":"<p>For support or questions, please contact: shahriarshm81@gmail.com Feel free to let me know if there are any additional details or changes you'd like to make!</p>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#016-2024-07-25","title":"0.1.6 - 2024-07-25","text":""},{"location":"changelog/#fixed","title":"Fixed","text":"<ul> <li>Fix misspell in ParsiNLUMultipleChoice task name.</li> <li>Fix wrong target key in the XLSummary.</li> <li>Add org prefix to the sentiment analysis task.</li> <li>Fix FarsTailEntailment prompt target key.</li> </ul>"},{"location":"changelog/#added","title":"Added","text":"<ul> <li>Add <code>attention_mask</code> to the transformer model <code>generate</code> function.</li> </ul>"},{"location":"changelog/#015-2024-07-18","title":"0.1.5 - 2024-07-18","text":""},{"location":"changelog/#added_1","title":"Added","text":"<ul> <li>Add FarsTail entailment task.</li> <li>Add Persian News Summary task.</li> <li>Add XL-Sum task.</li> <li>Add ParsiNLUBenchmark. It is a sub class of CustomBenchmark with hard-coded ParsiNLU tasks.</li> </ul>"},{"location":"changelog/#014-2024-07-12","title":"0.1.4 - 2024-07-12","text":""},{"location":"changelog/#fixed_1","title":"Fixed","text":"<ul> <li>Fix <code>load_all_tasks</code> returning empty list.</li> </ul>"},{"location":"changelog/#added_2","title":"Added","text":"<ul> <li>Add Anthropic model interface.</li> <li>Add retry on rate limit to API-based models.</li> <li>Add <code>skip_existing_matches</code> to the task evaluate function. It skips matches that are already generated and scored.</li> </ul>"},{"location":"changelog/#013-2024-07-06","title":"0.1.3 - 2024-07-06","text":""},{"location":"changelog/#fixed_2","title":"Fixed","text":"<ul> <li>Fix <code>model_name</code> property in PreTrainedTransformerModel.</li> </ul>"},{"location":"changelog/#added_3","title":"Added","text":"<ul> <li>Add Persian MMLU (Khayyam Challenge) task.</li> <li>Add <code>select_sub_tasks</code> to the task class.</li> </ul>"},{"location":"changelog/#012-2024-07-06","title":"0.1.2 - 2024-07-06","text":""},{"location":"changelog/#fixed_3","title":"Fixed","text":"<ul> <li>Fix misspells and typos.</li> <li>Use<code>name_or_path</code> parameter as the <code>model_name</code> in PreTrainedTransformerModel.</li> </ul>"},{"location":"changelog/#changed","title":"Changed","text":"<ul> <li>Update sentiment analysis task prompt template.</li> </ul>"},{"location":"changelog/#added_4","title":"Added","text":"<ul> <li>Add <code>completion_formatter</code> to the model interfaces.</li> </ul>"},{"location":"changelog/#011-2024-07-05","title":"0.1.1 - 2024-07-05","text":""},{"location":"changelog/#added_5","title":"Added","text":"<ul> <li>Add support for Python &gt;= 3.10</li> <li>Add <code>prefer_concurrency</code> to the benchmark, task and models.</li> </ul>"},{"location":"changelog/#010-2024-07-03","title":"0.1.0 - 2024-07-03","text":"<p>ParsBench got alive!</p>"},{"location":"contribution/","title":"Contribution","text":"<p>Thank you for considering contributing to ParsBench! Your contributions will help improve this framework for benchmarking Large Language Models (LLMs) on Persian language tasks. Here are the guidelines for contributing to the project.</p>"},{"location":"contribution/#how-to-contribute","title":"How to Contribute","text":""},{"location":"contribution/#1-fix-bugs","title":"1. Fix Bugs","text":"<p>If you find a bug in ParsBench, you can contribute by:</p> <ul> <li>Reporting the bug in the Issue Tracker.</li> <li>Providing a detailed description of the bug, including steps to reproduce it, the expected behavior, and the actual behavior.</li> <li>Submitting a pull request (PR) with a fix for the bug. Please ensure your fix includes:</li> <li>Clear and concise code changes.</li> <li>Relevant tests to verify the fix.</li> <li>Updates to the documentation, if applicable.</li> </ul>"},{"location":"contribution/#2-add-features","title":"2. Add Features","text":"<p>If you have an idea for a new feature, you can contribute by:</p> <ul> <li>Proposing the feature in the Issue Tracker.</li> <li>Providing a detailed description of the feature, including its purpose and potential impact.</li> <li>Submitting a PR that implements the feature. Please ensure your implementation includes:</li> <li>Clear and concise code changes.</li> <li>Relevant tests to verify the feature.</li> <li>Updates to the documentation, including usage examples.</li> </ul>"},{"location":"contribution/#3-add-new-tasks-with-new-datasets","title":"3. Add New Tasks with New Datasets","text":"<p>To expand the capabilities of ParsBench, you can contribute by:</p> <ul> <li>Proposing a new task in the Issue Tracker.</li> <li>Providing a detailed description of the task and the dataset, including:</li> <li>The task's objective and relevance to Persian language processing.</li> <li>Details about the dataset, including its source, format, and any preprocessing steps.</li> <li>Submitting a PR that adds the new task and dataset. Please ensure your contribution includes:</li> <li>Clear and concise code changes.</li> <li>Integration of the dataset with existing benchmarking tools.</li> <li>Relevant tests to verify the new task.</li> <li>Updates to the documentation, including instructions on how to use the new task.</li> </ul>"},{"location":"contribution/#4-run-benchmarks-on-sota-models-or-fine-tuned-open-source-models","title":"4. Run Benchmarks on SoTA Models or Fine-tuned Open-source Models","text":"<p>You can help by running benchmarks on state-of-the-art (SoTA) models or fine-tuned open-source models and sharing the results. To contribute:</p> <ul> <li>Select a SoTA model or fine-tuned open-source model relevant to Persian language tasks.</li> <li>Run benchmarks using ParsBench.</li> <li>Share the results by submitting a report in the Issue Tracker. Include:</li> <li>Details about the model and any fine-tuning performed.</li> <li>Saved matches.</li> <li>Benchmarking results, including performance metrics and any observations.</li> <li>Suggestions for improvements, if any.</li> </ul>"},{"location":"contribution/#contribution-process","title":"Contribution Process","text":"<ol> <li>Fork the Repository: Fork the ParsBench repository to your GitHub account.</li> <li>Clone the Repository: Clone the forked repository to your local machine.</li> </ol> <pre><code>git clone https://github.com/shahriarshm/parsbench.git\ncd ParsBench\n</code></pre> <ol> <li>Create a Branch: Create a new branch for your contribution.</li> </ol> <pre><code>git checkout -b feature/your-feature-name\n</code></pre> <ol> <li>Make Changes: Implement your changes in the new branch.</li> <li>Commit Changes: Commit your changes with a meaningful commit message.</li> </ol> <pre><code>git add .\ngit commit -m \"Description of your changes\"\n</code></pre> <ol> <li>Push Changes: Push your changes to your forked repository.</li> </ol> <pre><code>git push origin feature/your-feature-name\n</code></pre> <ol> <li>Create a Pull Request: Open a pull request (PR) to the main repository. Provide a detailed description of your changes and any relevant information.</li> </ol>"},{"location":"contribution/#code-style-and-testing","title":"Code Style and Testing","text":"<ul> <li>Follow the existing code style and conventions used in the project.</li> <li>Write clear and concise code with appropriate comments.</li> <li>Include relevant tests for your contributions.</li> <li>Ensure all tests pass before submitting your PR.</li> </ul>"},{"location":"contribution/#getting-help","title":"Getting Help","text":"<p>If you have any questions or need help, feel free to reach out in the Issue Tracker.</p> <p>Thank you for your contributions!</p>"},{"location":"donation/","title":"Donation","text":"<p>ParsBench is a self-found project for my weekends. I developed it far here using volunteer help of companies and individuals. If you want to donate me, here are some ways.</p>"},{"location":"donation/#support-me","title":"Support Me","text":"<p>You can support me by donating any amount of money you want. I will spend it on paying APIs and GPUs :)</p> <ul> <li>Hamibash</li> </ul>"},{"location":"donation/#buy-me-a-coffee","title":"Buy Me a Coffee","text":"<p>Buy coffee for my internal engine. It's a fuel to my brain for higher speed of coding.</p> <ul> <li>Coffeete</li> </ul>"},{"location":"donation/#crypto","title":"Crypto","text":"<p>Most valuable donation for me is Crypto.</p> <ul> <li>BTC (Bitcoin): \u200d<code>bc1q5sfazp0a9ls8rdg0ql6u70fr5pss0krvmctp5f</code></li> <li>ETH (Ethereum): <code>0x58a64E699a75D4c2CAE31411036819A91C397693</code></li> <li>USDT (Tron): <code>TDZnM5qKyGFBNmuDuAicAkTtHrVT78i515</code></li> </ul>"},{"location":"donation/#tell-me-youve-donated","title":"Tell Me You've Donated","text":"<p>Tell me you have donated if you want. I will make a list of donators in the ParsBench pages and put your name there. Reach me at shahriarshm81@gmail.com</p>"},{"location":"getting-started/","title":"Getting Started","text":""},{"location":"getting-started/#installation","title":"Installation","text":"<p>Install Math Equivalence package manually:</p> <pre><code>pip install git+https://github.com/hendrycks/math.git\n</code></pre> <p>Install ParsBench using pip:</p> <pre><code>pip install parsbench\n</code></pre>"},{"location":"getting-started/#usage","title":"Usage","text":""},{"location":"getting-started/#evaluating-a-pretrained-model","title":"Evaluating a PreTrained Model","text":"<p>Load the pre-trained model and tokenizer from the HuggingFace and then, evaluate the model using the PersianMath task:</p> <pre><code>from transformers import AutoModelForCausalLM, AutoTokenizer\n\nfrom parsbench.models import PreTrainedTransformerModel\nfrom parsbench.tasks import PersianMath\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen2-72B-Instruct\",\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-72B-Instruct\")\n\ntf_model = PreTrainedTransformerModel(model=model, tokenizer=tokenizer)\n\nwith PersianMath() as task:\n    results = task.evaluate(tf_model)\n</code></pre>"},{"location":"getting-started/#benchmarking-multiple-models-with-multiple-tasks","title":"Benchmarking Multiple Models with Multiple Tasks","text":"<p>For example, we run our local models using Ollama:</p> <pre><code>ollama run qwen2\nollama run aya\n</code></pre> <p>Then we benchmark those models using the ParsBench.</p> <pre><code>from parsbench.benchmarks import CustomBenchmark\nfrom parsbench.models import OpenAIModel\nfrom parsbench.tasks import ParsiNLUMultipleChoice, PersianMath, ParsiNLUReadingComprehension\n\nqwen2_model = OpenAIModel(\n    api_base_url=\"http://localhost:11434/v1/\",\n    api_secret_key=\"ollama\",\n    model=\"qwen2:latest\",\n)\naya_model = OpenAIModel(\n    api_base_url=\"http://localhost:11434/v1/\",\n    api_secret_key=\"ollama\",\n    model=\"aya:latest\",\n)\n\nbenchmark = CustomBenchmark(\n    models=[qwen2_model, aya_model],\n    tasks=[\n        ParsiNLUMultipleChoice,\n        ParsiNLUReadingComprehension,\n        PersianMath,\n    ],\n)\nresult = benchmark.run(\n    prompt_lang=\"fa\",\n    prompt_shots=[0, 3],\n    n_first=100,\n    sort_by_score=True,\n)\nresult.show_radar_plot()\n</code></pre> <p></p>"},{"location":"code_reference/benchmarks/","title":"Benchmarks","text":"<p>               Bases: <code>ABC</code></p> <p>This abstract class defines the structure for a benchmarking task. Subclasses of Benchmark must implement the 'run' method, which takes in various parameters related to the benchmarking task and returns a BenchmarkResult object.</p> <p>Methods:</p> Name Description <code>run</code> <p>Abstract method that must be implemented by subclasses. It runs the benchmarking task with the given parameters and returns a BenchmarkResult object.</p> Source code in <code>parsbench/benchmarks/base.py</code> <pre><code>class Benchmark(ABC):\n    \"\"\"\n    This abstract class defines the structure for a benchmarking task. Subclasses of Benchmark must implement the 'run' method, which takes in various parameters related to the benchmarking task and returns a BenchmarkResult object.\n\n    Methods:\n        run: Abstract method that must be implemented by subclasses. It runs the benchmarking task with the given parameters and returns a BenchmarkResult object.\n    \"\"\"\n\n    @abstractmethod\n    def run(\n        self,\n        prompt_lang: str = \"fa\",\n        prompt_shots: list[int] | None = None,\n        n_first: int | None = None,\n        sort_by_score: bool = True,\n        save_matches: bool = False,\n        save_evaluation: bool = False,\n        save_benchmark: bool = False,\n        output_path: str = None,\n        skip_existing_matches: bool = False,\n        prefer_concurrency: bool = True,\n        n_workers: int = 4,\n    ) -&gt; BenchmarkResult:\n        \"\"\"\n        Abstract method that must be implemented by subclasses. It runs the benchmarking task with the given parameters and returns a BenchmarkResult object.\n\n        Parameters:\n            prompt_lang (str, optional): The language of the prompt (default is \"fa\").\n            prompt_shots (list[int], optional): The list of prompt shots to evaluate (default is None).\n            n_first (int, optional): The number of initial prompts to consider (default is 200).\n            sort_by_score (bool, optional): Whether to sort the model benchmarks by average score (default is True).\n            save_matches (bool, optional): Flag to save the generated matches (default is False).\n            save_evaluation (bool, optional): Flag to save the evaluation results (default is False).\n            skip_existing_matches (bool, optional): Flag to skip already generated matches in the output path (default is False).\n            output_path (str, optional): The output path to save the matches and evaluation results.\n            prefer_concurrency (bool, optional): The flag to use concurrent processing if the model and task support that (default is True).\n            n_workers (int, optional): The number of workers for concurrent processing (default is 4).\n\n        Returns:\n            BenchmarkResult: An object containing the benchmarking results.\n        \"\"\"\n        pass\n</code></pre> <p>               Bases: <code>Benchmark</code></p> <p>CustomBenchmark class represents a custom benchmarking task that extends the Benchmark abstract class. It defines the run method to execute the benchmarking process for a given list of models and tasks.</p> <p>Attributes:</p> Name Type Description <code>models</code> <code>list[Model]</code> <p>The list of models to evaluate in the benchmarking task.</p> <code>tasks</code> <code>list[Task]</code> <p>The list of tasks to evaluate with the models.</p> <p>Methods:</p> Name Description <code>run</code> <p>Executes the benchmarking process for the specified models and tasks, generating evaluation results for each model on each task.</p> Source code in <code>parsbench/benchmarks/custom_benchmark.py</code> <pre><code>class CustomBenchmark(Benchmark):\n    \"\"\"\n    CustomBenchmark class represents a custom benchmarking task that extends the Benchmark abstract class. It defines the run method to execute the benchmarking process for a given list of models and tasks.\n\n    Attributes:\n        models (list[Model]): The list of models to evaluate in the benchmarking task.\n        tasks (list[Task]): The list of tasks to evaluate with the models.\n\n    Methods:\n        run: Executes the benchmarking process for the specified models and tasks, generating evaluation results for each model on each task.\n    \"\"\"\n\n    def __init__(\n        self,\n        models: list[Model],\n        tasks: list[Task],\n    ):\n        self.models = models\n        self.tasks = tasks\n\n    def run(\n        self,\n        prompt_lang: str = \"fa\",\n        prompt_shots: list[int] | None = None,\n        n_first: int | None = None,\n        sort_by_score: bool = True,\n        save_matches: bool = False,\n        save_evaluation: bool = False,\n        save_benchmark: bool = False,\n        output_path: str = None,\n        skip_existing_matches: bool = False,\n        prefer_concurrency: bool = True,\n        n_workers: int = 4,\n    ) -&gt; BenchmarkResult:\n        \"\"\"\n        Run the benchmarking process for the given models and tasks.\n\n        Parameters:\n            prompt_lang (str, optional): The language of the prompt (default is \"fa\").\n            prompt_shots (list[int], optional): The list of prompt shots to evaluate (default is None).\n            n_first (int, optional): The number of initial prompts to consider (default is 200).\n            sort_by_score (bool, optional): Whether to sort the model benchmarks by average score (default is True).\n            save_matches (bool, optional): Flag to save the generated matches (default is False).\n            save_evaluation (bool, optional): Flag to save the evaluation results (default is False).\n            output_path (str, optional): The output path to save the matches and evaluation results.\n            skip_existing_matches (bool, optional): Flag to skip already generated matches in the output path (default is False).\n            prefer_concurrency (bool, optional): The flag to use concurrent processing if the model and task support that (default is True).\n            n_workers (int, optional): The number of workers for concurrent processing (default is 4).\n\n        Returns:\n            BenchmarkResult: The result of the benchmarking process.\n        \"\"\"\n\n        model_evaluations: dict[str, list[EvaluationResult]] = defaultdict(list)\n\n        for task in self.tasks:\n            print(f\"Evaluating {task.task_name}:\")\n\n            if inspect.isclass(task):\n                if issubclass(task, Task):\n                    task: Task = task()\n                else:\n                    raise TypeError(\n                        f\"{task} is not a subclass/instance of the Task class.\"\n                    )\n\n            with task:\n                for model in self.models:\n                    print(f\"Model: {model.model_name}\")\n\n                    evaluation_results = task.evaluate(\n                        model=model,\n                        prompt_lang=prompt_lang,\n                        prompt_shots=prompt_shots,\n                        n_first=n_first,\n                        save_matches=save_matches,\n                        save_evaluation=save_evaluation,\n                        output_path=output_path,\n                        skip_existing_matches=skip_existing_matches,\n                        prefer_concurrency=prefer_concurrency,\n                        n_workers=n_workers,\n                    )\n                    model_evaluations[model.model_name].extend(evaluation_results)\n\n        model_benchmarks = [\n            ModelBenchmarkResult(\n                model_name=model_name,\n                evaluation_results=evaluation_results,\n            )\n            for model_name, evaluation_results in model_evaluations.items()\n        ]\n\n        if sort_by_score:\n            model_benchmarks.sort(key=lambda mb: mb.average_score, reverse=True)\n\n        benchmark_result = BenchmarkResult(model_benchmarks=model_benchmarks)\n\n        if save_benchmark:\n            benchmark_result.save(output_path)\n\n        return benchmark_result\n</code></pre> <p>               Bases: <code>CustomBenchmark</code></p> <p>This benchmark class includes all existing tasks which use ParsiNLU datasets.</p> <p>Attributes:</p> Name Type Description <code>models</code> <code>list[Model]</code> <p>The list of models to evaluate in the benchmarking task.</p> <p>Methods:</p> Name Description <code>run</code> <p>Executes the benchmarking process for the specified models, generating evaluation results for each model on each task.</p> Source code in <code>parsbench/benchmarks/parsinlu_benchmark.py</code> <pre><code>class ParsiNLUBenchmark(CustomBenchmark):\n    \"\"\"\n    This benchmark class includes all existing tasks which use ParsiNLU datasets.\n\n    Attributes:\n        models (list[Model]): The list of models to evaluate in the benchmarking task.\n\n    Methods:\n        run: Executes the benchmarking process for the specified models, generating evaluation results for each model on each task.\n    \"\"\"\n\n    def __init__(self, models: list[Model]):\n        tasks = [\n            ParsiNLUEntailment,\n            ParsiNLUMachineTranslationEnFa,\n            ParsiNLUMachineTranslationFaEn,\n            ParsiNLUMultipleChoice,\n            ParsiNLUReadingComprehension,\n            ParsiNLUSentimentAnalysis,\n        ]\n        super().__init__(models, tasks)\n</code></pre> <p>Represents the results of benchmarking a model across multiple evaluations.</p> <p>Attributes:</p> Name Type Description <code>model_name</code> <code>str</code> <p>The name of the model being benchmarked.</p> <code>evaluation_results</code> <code>list[EvaluationResult]</code> <p>A list of EvaluationResult objects representing the evaluation results for the model.</p> Source code in <code>parsbench/benchmarks/benchmark_result.py</code> <pre><code>@dataclass\nclass ModelBenchmarkResult:\n    \"\"\"\n    Represents the results of benchmarking a model across multiple evaluations.\n\n    Attributes:\n        model_name (str): The name of the model being benchmarked.\n        evaluation_results (list[EvaluationResult]): A list of EvaluationResult objects representing the evaluation results for the model.\n    \"\"\"\n\n    model_name: str\n    evaluation_results: list[EvaluationResult]\n\n    @classmethod\n    def from_dict(cls, data: dict) -&gt; \"ModelBenchmarkResult\":\n        evaluation_results = [\n            EvaluationResult.from_dict(task) for task in data.pop(\"evaluation_results\")\n        ]\n        return cls(**data, evaluation_results=evaluation_results)\n\n    def to_dict(self) -&gt; dict:\n        return {\n            **asdict(self),\n            \"evaluation_results\": [e.to_dict() for e in self.evaluation_results],\n        }\n\n    def to_pandas(self) -&gt; pd.DataFrame:\n        return pd.concat([er.to_pandas() for er in self.evaluation_results])\n\n    def __str__(self) -&gt; str:\n        text = f\"Model: {self.model_name}\\nEvaluation Results:\\n\"\n        for er in self.evaluation_results:\n            text += f\"- {er.task_name}\"\n            if er.sub_task:\n                text += f\" ({er.sub_task}):\\n\"\n            else:\n                text += \":\\n\"\n            for psr in er.prompt_shot_results:\n                text += f\"  - {psr.n_shots}-shot prompt: {psr.score:.4f}\\n\"\n        return text.strip(\"\\n\")\n\n    @property\n    def average_score(self) -&gt; float:\n        return sum([er.average_score for er in self.evaluation_results]) / len(\n            self.evaluation_results\n        )\n</code></pre> <p>Represents the results of benchmarking multiple models across various evaluations.</p> <p>Attributes:</p> Name Type Description <code>model_benchmarks</code> <code>list[ModelBenchmarkResult]</code> <p>A list of ModelBenchmarkResult objects representing the benchmark results for each model.</p> Source code in <code>parsbench/benchmarks/benchmark_result.py</code> <pre><code>@dataclass\nclass BenchmarkResult:\n    \"\"\"\n    Represents the results of benchmarking multiple models across various evaluations.\n\n    Attributes:\n        model_benchmarks (list[ModelBenchmarkResult]): A list of ModelBenchmarkResult objects representing the benchmark results for each model.\n    \"\"\"\n\n    model_benchmarks: list[ModelBenchmarkResult]\n\n    @classmethod\n    def from_file(cls, path: str) -&gt; \"BenchmarkResult\":\n        with jsonlines.open(path, \"r\") as reader:\n            model_benchmarks: list[ModelBenchmarkResult] = []\n            for row in reader.iter(type=dict, skip_invalid=True):\n                model_benchmarks.append(ModelBenchmarkResult.from_dict(row))\n\n        return cls(model_benchmarks=model_benchmarks)\n\n    @classmethod\n    def from_evaluation_files(cls, path: str) -&gt; \"BenchmarkResult\":\n        models = [(d.name, d.path) for d in os.scandir(path) if d.is_dir()]\n        model_benchmarks = []\n\n        for model_name, model_path in models:\n            eval_paths = [d.path for d in os.scandir(model_path) if d.is_dir()]\n\n            evaluation_results = []\n\n            for eval_path in eval_paths:\n                eval_files = [\n                    f\n                    for f in os.scandir(eval_path)\n                    if f.is_file() and f.name.startswith(\"evaluation\")\n                ]\n                evaluation_results.extend(\n                    [\n                        EvaluationResult.from_file(eval_file.path)\n                        for eval_file in eval_files\n                    ]\n                )\n\n            model_benchmarks.append(\n                ModelBenchmarkResult(\n                    model_name=model_name,\n                    evaluation_results=evaluation_results,\n                )\n            )\n\n        return BenchmarkResult(model_benchmarks=model_benchmarks)\n\n    @classmethod\n    def from_matches_files(cls, path: str, rescore: bool = False) -&gt; \"BenchmarkResult\":\n        task_cls_mapping = {\n            task_cls.task_name.replace(\"-\", \" \"): task_cls\n            for task_cls in load_all_tasks()\n        }\n\n        _with_subtask_pattern = re.compile(r\"matches_([\\w\\s]+)_(\\d+)_shot\\.jsonl\")\n        _without_subtask_pattern = re.compile(r\"matches_(\\d+)_shot\\.jsonl\")\n\n        matches_paths = glob.glob(f\"{path}/*/*/matches*.jsonl\")\n\n        model_evals: list[tuple[str, str, str, TaskMatchGroup]] = []\n\n        for match_path in matches_paths:\n            match_file = os.path.basename(match_path)\n            task_name = os.path.basename(os.path.dirname(match_path)).replace(\"_\", \" \")\n            model_name = os.path.basename(os.path.dirname(os.path.dirname(match_path)))\n            sub_task = None\n            n_shots = 0\n\n            if m := _with_subtask_pattern.match(match_file):\n                sub_task = m.group(1)\n                n_shots = int(m.group(2))\n            elif m := _without_subtask_pattern.match(match_file):\n                n_shots = int(m.group(1))\n            else:\n                raise Exception(\n                    f\"Matches file '{match_file}' doesn't match the expected pattern.\"\n                )\n\n            task_matches = TaskMatchGroup.from_file(\n                Path(match_path).parent, n_shots=n_shots, sub_task=sub_task\n            )\n            assert (\n                task_name is not task_cls_mapping\n            ), f\"No task class found for '{task_name}'.\"\n\n            model_evals.append((model_name, task_name, sub_task, task_matches))\n\n        model_benchmarks: list[ModelBenchmarkResult] = []\n\n        for model_name, task_evals in itertools.groupby(\n            model_evals, key=lambda t: t[0]\n        ):\n            print(f\"Model: {model_name}\")\n            evaluation_results: list[EvaluationResult] = []\n\n            for task_name, task_matches_group in itertools.groupby(\n                task_evals, key=lambda t: t[1]\n            ):\n                print(f\"Re-scoring {task_name}:\")\n                task = task_cls_mapping[task_name]()\n                prompt_shot_evals = defaultdict(list)\n\n                for _, _, sub_task, task_matches in task_matches_group:\n                    print(f\"{sub_task} {task_matches.n_shots}-shot prompt:\")\n                    if rescore:\n                        task_matches = task.score_matches(task_matches)\n\n                    score = task.get_overall_score(task_matches)\n\n                    prompt_shot_evals[sub_task].append(\n                        PromptShotEvaluationResult(\n                            n_shots=task_matches.n_shots, score=score\n                        )\n                    )\n\n                evaluation_results.extend(\n                    EvaluationResult(\n                        model_name=model_name,\n                        task_name=task_name,\n                        task_category=task.task_category,\n                        score_name=task.score_name,\n                        prompt_shot_results=prompt_shot_results,\n                        sub_task=sub_task,\n                    )\n                    for sub_task, prompt_shot_results in prompt_shot_evals.items()\n                )\n\n            model_benchmarks.append(\n                ModelBenchmarkResult(\n                    model_name=model_name, evaluation_results=evaluation_results\n                )\n            )\n            print(\"-\" * 10)\n\n        return BenchmarkResult(model_benchmarks=model_benchmarks)\n\n    @classmethod\n    def from_dict(cls, data: dict) -&gt; \"BenchmarkResult\":\n        model_benchmarks = [\n            ModelBenchmarkResult.from_dict(mbr) for mbr in data.pop(\"model_benchmarks\")\n        ]\n        return cls(**data, model_benchmarks=model_benchmarks)\n\n    def to_dict(self) -&gt; dict:\n        return {\n            **asdict(self),\n            \"model_benchmarks\": [mb.to_dict() for mb in self.model_benchmarks],\n        }\n\n    def to_pandas(self, pivot: bool = False) -&gt; pd.DataFrame:\n        df = pd.concat([mb.to_pandas() for mb in self.model_benchmarks])\n        if pivot:\n            return df.pivot(\n                index=[\"task_category\", \"task_name\", \"sub_task\", \"score_name\"],\n                columns=[\"model_name\", \"n_shots\"],\n                values=[\"score\"],\n            )\n        return df\n\n    def show_radar_plot(self, title=\"Radar Plot\"):\n        data = []\n        categories = set()\n\n        for mb in self.model_benchmarks:\n            values = []\n            for _, evals in groupby(mb.evaluation_results, key=lambda e: e.task_name):\n                evals = list(evals)\n                score = sum(e.average_score for e in evals) / len(evals)\n                values.append(score)\n\n            data.append({\"name\": mb.model_name, \"values\": values})\n            categories |= set(e.task_name for e in mb.evaluation_results)\n\n        _radar_plot(data, categories, title)\n\n    def save(self, path: str):\n        benchmark_path = Path(path) / \"benchmark.jsonl\"\n\n        with jsonlines.open(benchmark_path, \"w\") as writer:\n            for mb in self.model_benchmarks:\n                writer.write(mb.to_dict())\n\n    def __str__(self) -&gt; str:\n        text = \"\"\n        for mb in self.model_benchmarks:\n            text += str(mb) + \"\\n\" + \"-\" * 10 + \"\\n\"\n        return text.strip(\"\\n\")\n</code></pre> <p>Merge multiple BenchmarkResult objects into a single BenchmarkResult object.</p> <p>Parameters:</p> Name Type Description Default <code>benchmarks</code> <code>list[BenchmarkResult]</code> <p>A list of BenchmarkResult objects to merge.</p> required <code>sort</code> <code>bool</code> <p>Whether to sort the merged ModelBenchmarkResult list by average score. Defaults to True.</p> <code>True</code> <code>keep_duplicates</code> <code>bool</code> <p>Whether to keep duplicate model names in the merged list. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>BenchmarkResult</code> <code>BenchmarkResult</code> <p>A new BenchmarkResult object containing the merged ModelBenchmarkResult list.</p> Source code in <code>parsbench/benchmarks/benchmark_result.py</code> <pre><code>def merge_benchmark_results(\n    benchmarks: list[BenchmarkResult], sort: bool = True, keep_duplicates: bool = False\n) -&gt; BenchmarkResult:\n    \"\"\"\n    Merge multiple BenchmarkResult objects into a single BenchmarkResult object.\n\n    Parameters:\n        benchmarks (list[BenchmarkResult]): A list of BenchmarkResult objects to merge.\n        sort (bool, optional): Whether to sort the merged ModelBenchmarkResult list by average score. Defaults to True.\n        keep_duplicates (bool, optional): Whether to keep duplicate model names in the merged list. Defaults to False.\n\n    Returns:\n        BenchmarkResult: A new BenchmarkResult object containing the merged ModelBenchmarkResult list.\n    \"\"\"\n    model_benchmarks: list[ModelBenchmarkResult] = []\n    for benchmark in benchmarks:\n        model_benchmarks.extend(benchmark.model_benchmarks)\n\n    if not keep_duplicates:\n        model_names = set()\n        skipped = 0\n        for index in range(len(model_benchmarks)):\n            mbr = model_benchmarks[index - skipped]\n            if mbr.model_name in model_names:\n                skipped += 1\n                model_benchmarks.pop(index - skipped)\n            model_names.add(mbr.model_name)\n\n    if sort:\n        model_benchmarks.sort(key=lambda m: m.average_score, reverse=True)\n\n    return BenchmarkResult(model_benchmarks=model_benchmarks)\n</code></pre> <p>This function generates leaderboard data from the benchmark result object.</p> <p>Parameters:</p> Name Type Description Default <code>benchmark_result</code> <code>BenchmarkResult</code> <p>BenchmarkResult object.</p> required <code>leaderboard_path</code> <code>str</code> <p>Path to store the leaderboard data.</p> required Source code in <code>parsbench/benchmarks/benchmark_result.py</code> <pre><code>def build_leaderboard_from_benchmark(\n    benchmark_result: BenchmarkResult, leaderboard_path: str\n):\n    \"\"\"\n    This function generates leaderboard data from the benchmark result object.\n\n    Parameters:\n        benchmark_result (BenchmarkResult): BenchmarkResult object.\n        leaderboard_path (str): Path to store the leaderboard data.\n    \"\"\"\n    requests_path = Path(leaderboard_path) / \"requests\"\n    results_path = Path(leaderboard_path) / \"results\"\n\n    requests_path.mkdir(exist_ok=True)\n    results_path.mkdir(exist_ok=True)\n\n    now = datetime.datetime.now(pytz.UTC).isoformat(timespec=\"seconds\")\n\n    for mb in benchmark_result.model_benchmarks:\n        model_name = mb.model_name\n\n        os.makedirs(results_path / model_name, exist_ok=True)\n\n        request = {\n            \"model\": model_name,\n            \"base_model\": \"\",\n            \"revision\": \"main\",\n            \"private\": False,\n            \"precision\": \"?\",\n            \"weight_type\": \"Original\",\n            \"status\": \"FINISHED\",\n            \"submitted_time\": now,\n            \"model_type\": \"\\ud83d\\udfe2 : pretrained\",\n            \"likes\": 0,\n            \"params\": 0.1,\n            \"license\": \"custom\",\n        }\n        with open(\n            requests_path / f\"{model_name}_eval_request_nshot.json\", \"wt\"\n        ) as writer:\n            writer.write(json.dumps(request))\n\n        result = {\n            \"config\": {\"model_dtype\": \"\", \"model_name\": model_name, \"model_sha\": \"\"},\n            \"results\": {\n                er.task_name: {er.score_name: round(er.max_score, 3)}\n                for er in mb.evaluation_results\n            },\n        }\n\n        with open(results_path / model_name / f\"results_{now}.json\", \"wt\") as writer:\n            writer.write(json.dumps(result))\n</code></pre>"},{"location":"code_reference/benchmarks/#parsbench.benchmarks.base.Benchmark.run","title":"<code>run(prompt_lang='fa', prompt_shots=None, n_first=None, sort_by_score=True, save_matches=False, save_evaluation=False, save_benchmark=False, output_path=None, skip_existing_matches=False, prefer_concurrency=True, n_workers=4)</code>  <code>abstractmethod</code>","text":"<p>Abstract method that must be implemented by subclasses. It runs the benchmarking task with the given parameters and returns a BenchmarkResult object.</p> <p>Parameters:</p> Name Type Description Default <code>prompt_lang</code> <code>str</code> <p>The language of the prompt (default is \"fa\").</p> <code>'fa'</code> <code>prompt_shots</code> <code>list[int]</code> <p>The list of prompt shots to evaluate (default is None).</p> <code>None</code> <code>n_first</code> <code>int</code> <p>The number of initial prompts to consider (default is 200).</p> <code>None</code> <code>sort_by_score</code> <code>bool</code> <p>Whether to sort the model benchmarks by average score (default is True).</p> <code>True</code> <code>save_matches</code> <code>bool</code> <p>Flag to save the generated matches (default is False).</p> <code>False</code> <code>save_evaluation</code> <code>bool</code> <p>Flag to save the evaluation results (default is False).</p> <code>False</code> <code>skip_existing_matches</code> <code>bool</code> <p>Flag to skip already generated matches in the output path (default is False).</p> <code>False</code> <code>output_path</code> <code>str</code> <p>The output path to save the matches and evaluation results.</p> <code>None</code> <code>prefer_concurrency</code> <code>bool</code> <p>The flag to use concurrent processing if the model and task support that (default is True).</p> <code>True</code> <code>n_workers</code> <code>int</code> <p>The number of workers for concurrent processing (default is 4).</p> <code>4</code> <p>Returns:</p> Name Type Description <code>BenchmarkResult</code> <code>BenchmarkResult</code> <p>An object containing the benchmarking results.</p> Source code in <code>parsbench/benchmarks/base.py</code> <pre><code>@abstractmethod\ndef run(\n    self,\n    prompt_lang: str = \"fa\",\n    prompt_shots: list[int] | None = None,\n    n_first: int | None = None,\n    sort_by_score: bool = True,\n    save_matches: bool = False,\n    save_evaluation: bool = False,\n    save_benchmark: bool = False,\n    output_path: str = None,\n    skip_existing_matches: bool = False,\n    prefer_concurrency: bool = True,\n    n_workers: int = 4,\n) -&gt; BenchmarkResult:\n    \"\"\"\n    Abstract method that must be implemented by subclasses. It runs the benchmarking task with the given parameters and returns a BenchmarkResult object.\n\n    Parameters:\n        prompt_lang (str, optional): The language of the prompt (default is \"fa\").\n        prompt_shots (list[int], optional): The list of prompt shots to evaluate (default is None).\n        n_first (int, optional): The number of initial prompts to consider (default is 200).\n        sort_by_score (bool, optional): Whether to sort the model benchmarks by average score (default is True).\n        save_matches (bool, optional): Flag to save the generated matches (default is False).\n        save_evaluation (bool, optional): Flag to save the evaluation results (default is False).\n        skip_existing_matches (bool, optional): Flag to skip already generated matches in the output path (default is False).\n        output_path (str, optional): The output path to save the matches and evaluation results.\n        prefer_concurrency (bool, optional): The flag to use concurrent processing if the model and task support that (default is True).\n        n_workers (int, optional): The number of workers for concurrent processing (default is 4).\n\n    Returns:\n        BenchmarkResult: An object containing the benchmarking results.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"code_reference/benchmarks/#parsbench.benchmarks.custom_benchmark.CustomBenchmark.run","title":"<code>run(prompt_lang='fa', prompt_shots=None, n_first=None, sort_by_score=True, save_matches=False, save_evaluation=False, save_benchmark=False, output_path=None, skip_existing_matches=False, prefer_concurrency=True, n_workers=4)</code>","text":"<p>Run the benchmarking process for the given models and tasks.</p> <p>Parameters:</p> Name Type Description Default <code>prompt_lang</code> <code>str</code> <p>The language of the prompt (default is \"fa\").</p> <code>'fa'</code> <code>prompt_shots</code> <code>list[int]</code> <p>The list of prompt shots to evaluate (default is None).</p> <code>None</code> <code>n_first</code> <code>int</code> <p>The number of initial prompts to consider (default is 200).</p> <code>None</code> <code>sort_by_score</code> <code>bool</code> <p>Whether to sort the model benchmarks by average score (default is True).</p> <code>True</code> <code>save_matches</code> <code>bool</code> <p>Flag to save the generated matches (default is False).</p> <code>False</code> <code>save_evaluation</code> <code>bool</code> <p>Flag to save the evaluation results (default is False).</p> <code>False</code> <code>output_path</code> <code>str</code> <p>The output path to save the matches and evaluation results.</p> <code>None</code> <code>skip_existing_matches</code> <code>bool</code> <p>Flag to skip already generated matches in the output path (default is False).</p> <code>False</code> <code>prefer_concurrency</code> <code>bool</code> <p>The flag to use concurrent processing if the model and task support that (default is True).</p> <code>True</code> <code>n_workers</code> <code>int</code> <p>The number of workers for concurrent processing (default is 4).</p> <code>4</code> <p>Returns:</p> Name Type Description <code>BenchmarkResult</code> <code>BenchmarkResult</code> <p>The result of the benchmarking process.</p> Source code in <code>parsbench/benchmarks/custom_benchmark.py</code> <pre><code>def run(\n    self,\n    prompt_lang: str = \"fa\",\n    prompt_shots: list[int] | None = None,\n    n_first: int | None = None,\n    sort_by_score: bool = True,\n    save_matches: bool = False,\n    save_evaluation: bool = False,\n    save_benchmark: bool = False,\n    output_path: str = None,\n    skip_existing_matches: bool = False,\n    prefer_concurrency: bool = True,\n    n_workers: int = 4,\n) -&gt; BenchmarkResult:\n    \"\"\"\n    Run the benchmarking process for the given models and tasks.\n\n    Parameters:\n        prompt_lang (str, optional): The language of the prompt (default is \"fa\").\n        prompt_shots (list[int], optional): The list of prompt shots to evaluate (default is None).\n        n_first (int, optional): The number of initial prompts to consider (default is 200).\n        sort_by_score (bool, optional): Whether to sort the model benchmarks by average score (default is True).\n        save_matches (bool, optional): Flag to save the generated matches (default is False).\n        save_evaluation (bool, optional): Flag to save the evaluation results (default is False).\n        output_path (str, optional): The output path to save the matches and evaluation results.\n        skip_existing_matches (bool, optional): Flag to skip already generated matches in the output path (default is False).\n        prefer_concurrency (bool, optional): The flag to use concurrent processing if the model and task support that (default is True).\n        n_workers (int, optional): The number of workers for concurrent processing (default is 4).\n\n    Returns:\n        BenchmarkResult: The result of the benchmarking process.\n    \"\"\"\n\n    model_evaluations: dict[str, list[EvaluationResult]] = defaultdict(list)\n\n    for task in self.tasks:\n        print(f\"Evaluating {task.task_name}:\")\n\n        if inspect.isclass(task):\n            if issubclass(task, Task):\n                task: Task = task()\n            else:\n                raise TypeError(\n                    f\"{task} is not a subclass/instance of the Task class.\"\n                )\n\n        with task:\n            for model in self.models:\n                print(f\"Model: {model.model_name}\")\n\n                evaluation_results = task.evaluate(\n                    model=model,\n                    prompt_lang=prompt_lang,\n                    prompt_shots=prompt_shots,\n                    n_first=n_first,\n                    save_matches=save_matches,\n                    save_evaluation=save_evaluation,\n                    output_path=output_path,\n                    skip_existing_matches=skip_existing_matches,\n                    prefer_concurrency=prefer_concurrency,\n                    n_workers=n_workers,\n                )\n                model_evaluations[model.model_name].extend(evaluation_results)\n\n    model_benchmarks = [\n        ModelBenchmarkResult(\n            model_name=model_name,\n            evaluation_results=evaluation_results,\n        )\n        for model_name, evaluation_results in model_evaluations.items()\n    ]\n\n    if sort_by_score:\n        model_benchmarks.sort(key=lambda mb: mb.average_score, reverse=True)\n\n    benchmark_result = BenchmarkResult(model_benchmarks=model_benchmarks)\n\n    if save_benchmark:\n        benchmark_result.save(output_path)\n\n    return benchmark_result\n</code></pre>"},{"location":"code_reference/models/","title":"Models","text":"<p>               Bases: <code>ABC</code></p> <p>An abstract base class representing a model.</p> <p>Attributes:</p> Name Type Description <code>model_name</code> <code>property</code> <p>A property representing the name of the model.</p> <code>support_concurrency</code> <code>bool</code> <p>A flag indicating if the model supports concurrency.</p> <p>Methods:</p> Name Description <code>model_name</code> <p>Abstract method to return the name of the model.</p> <code>get_prompt_completion </code> <p>str) -&gt; str: Abstract method to generate completion for a given prompt.</p> <code>prompt_formatter </code> <p>str) -&gt; Union[str, List[Dict]]: Abstract method to format a prompt.</p> <code>completion_formatter </code> <p>str) -&gt; str: Method to format the model completion.</p> <code>generate_completions </code> <p>TaskMatchGroup, prefer_concurrency: bool = True, n_workers: int = 4) -&gt; TaskMatchGroup: Method to generate completions for a list of matches, optionally using concurrency.</p> Note <p>This class should be subclassed to implement the abstract methods.</p> Source code in <code>parsbench/models/base.py</code> <pre><code>class Model(ABC):\n    \"\"\"\n    An abstract base class representing a model.\n\n    Attributes:\n        model_name (property): A property representing the name of the model.\n        support_concurrency (bool): A flag indicating if the model supports concurrency.\n\n    Methods:\n        model_name(self) -&gt; str: Abstract method to return the name of the model.\n        get_prompt_completion (self, prompt: str) -&gt; str: Abstract method to generate completion for a given prompt.\n        prompt_formatter (self, prompt: str) -&gt; Union[str, List[Dict]]: Abstract method to format a prompt.\n        completion_formatter (self, completion: str) -&gt; str: Method to format the model completion.\n        generate_completions (self, matches: TaskMatchGroup, prefer_concurrency: bool = True, n_workers: int = 4) -&gt; TaskMatchGroup: Method to generate completions for a list of matches, optionally using concurrency.\n\n    Note:\n        This class should be subclassed to implement the abstract methods.\n    \"\"\"\n\n    support_concurrency: bool = False\n\n    @property\n    @abstractmethod\n    def model_name(self) -&gt; str:\n        pass\n\n    @abstractmethod\n    def get_prompt_completion(self, prompt: str) -&gt; str:\n        pass\n\n    @abstractmethod\n    def prompt_formatter(self, prompt: str) -&gt; str | list[dict]:\n        pass\n\n    def completion_formatter(self, completion: str) -&gt; str:\n        return completion\n\n    def generate_completions(\n        self,\n        matches: \"TaskMatchGroup\",\n        prefer_concurrency: bool = True,\n        skip_existing: bool = False,\n        n_workers: int = 4,\n    ) -&gt; \"TaskMatchGroup\":\n        if prefer_concurrency and self.support_concurrency:\n            matches = self._gen_with_concurrency(\n                matches, n_workers=n_workers, skip_existing=skip_existing\n            )\n        else:\n            for match in tqdm(\n                matches, total=len(matches), desc=\"Generating completions\"\n            ):\n                if match.completion is not None and skip_existing:\n                    continue\n                match.completion = self.completion_formatter(\n                    self.get_prompt_completion(match.prompt)\n                )\n        return matches\n\n    def _gen_with_concurrency(\n        self,\n        matches: \"TaskMatchGroup\",\n        n_workers: int = 4,\n        skip_existing: bool = False,\n    ) -&gt; \"TaskMatchGroup\":\n        def _gen_single_match_completion(match: \"TaskMatch\") -&gt; \"TaskMatch\":\n            match.completion = self.completion_formatter(\n                self.get_prompt_completion(match.prompt)\n            )\n            return match\n\n        with ThreadPoolExecutor(max_workers=n_workers) as executor:\n            futures = []\n\n            for match in matches:\n                if match.completion is not None and skip_existing:\n                    continue\n                future = executor.submit(\n                    _gen_single_match_completion,\n                    match,\n                )\n                futures.append(future)\n\n            for future in tqdm(\n                as_completed(futures), total=len(futures), desc=\"Generating completions\"\n            ):\n                future.result()\n\n        matches.matches.sort(key=lambda m: m.id)\n        return matches\n</code></pre> <p>               Bases: <code>Model</code></p> <p>A model interface for OpenAI-like APIs.</p> <p>Attributes:</p> Name Type Description <code>api_base_url</code> <code>str</code> <p>The base URL for the OpenAI API.</p> <code>api_secret_key</code> <code>str</code> <p>The secret key for accessing the OpenAI API.</p> <code>model</code> <code>str</code> <p>The specific model being used for processing.</p> <code>instruction_prompt</code> <code>str</code> <p>The default instruction prompt for the model.</p> <code>model_parameters</code> <code>dict</code> <p>Additional parameters specific to the model.</p> <code>completion_parameters</code> <code>dict</code> <p>Parameters for completion generation.</p> <code>retry_on_ratelimit</code> <p>bool = False,</p> <code>cooldown_interval</code> <p>int = 10,</p> <code>max_retries</code> <p>int = 1,</p> <code>client</code> <code>OpenAI</code> <p>An instance of the OpenAI client for API interactions.</p> <p>Methods:</p> Name Description <code>model_name</code> <p>Returns the name of the model.</p> <code>prompt_formatter</code> <p>Formats a given prompt into a list of messages. Could be overloaded.</p> <code>completion_formatter</code> <p>Method to format the model completion. Could be overloaded.</p> <code>get_prompt_completion</code> <p>Generates completion for a given prompt using the OpenAI API.</p> <code>generate_completions</code> <p>Generates completions for a list of TaskMatch objects using ThreadPoolExecutor.</p> Source code in <code>parsbench/models/openai_interface.py</code> <pre><code>class OpenAIModel(Model):\n    \"\"\"\n    A model interface for OpenAI-like APIs.\n\n    Attributes:\n        api_base_url (str): The base URL for the OpenAI API.\n        api_secret_key (str): The secret key for accessing the OpenAI API.\n        model (str): The specific model being used for processing.\n        instruction_prompt (str): The default instruction prompt for the model.\n        model_parameters (dict): Additional parameters specific to the model.\n        completion_parameters (dict): Parameters for completion generation.\n        retry_on_ratelimit: bool = False,\n        cooldown_interval: int = 10,\n        max_retries: int = 1,\n        client (OpenAI): An instance of the OpenAI client for API interactions.\n\n    Methods:\n        model_name: Returns the name of the model.\n        prompt_formatter: Formats a given prompt into a list of messages. Could be overloaded.\n        completion_formatter: Method to format the model completion. Could be overloaded.\n        get_prompt_completion: Generates completion for a given prompt using the OpenAI API.\n        generate_completions: Generates completions for a list of TaskMatch objects using ThreadPoolExecutor.\n    \"\"\"\n\n    support_concurrency: bool = True\n\n    def __init__(\n        self,\n        api_base_url: str,\n        api_secret_key: str,\n        model: str,\n        instruction_prompt: str = DEFAULT_INSTRUCTION_PROMPT,\n        model_parameters: dict = None,\n        completion_parameters: dict = None,\n        retry_on_ratelimit: bool = False,\n        cooldown_interval: int = 10,\n        max_retries: int = 1,\n        **kwargs\n    ):\n        self.api_base_url = api_base_url\n        self.api_secret_key = api_secret_key\n        self.model = model\n        self.instruction_prompt = instruction_prompt\n        self.model_parameters = model_parameters or dict()\n        self.completion_parameters = completion_parameters or dict(temperature=0.7)\n        self.retry_on_ratelimit = retry_on_ratelimit\n        self.cooldown_interval = cooldown_interval\n        self.max_retries = max_retries\n\n        self.client = OpenAI(\n            base_url=self.api_base_url,\n            api_key=self.api_secret_key,\n            **self.model_parameters,\n        )\n\n    @property\n    def model_name(self) -&gt; str:\n        return self.model\n\n    def prompt_formatter(self, prompt: str) -&gt; list[dict]:\n        messages = [\n            {\"role\": \"system\", \"content\": self.instruction_prompt},\n            {\"role\": \"user\", \"content\": prompt},\n        ]\n        return messages\n\n    def get_prompt_completion(self, prompt: str) -&gt; str:\n        messages = self.prompt_formatter(prompt)\n\n        retries = 0\n        while retries &lt; self.max_retries:\n            try:\n                completion = self.client.chat.completions.create(\n                    model=self.model,\n                    messages=messages,\n                    **self.completion_parameters,\n                    stream=False,  # Always override this parameter.\n                )\n                return completion.choices[0].message.content\n            except RateLimitError as exc:\n                if self.retry_on_ratelimit:\n                    retries += 1\n                    time.sleep(self.cooldown_interval)\n                else:\n                    raise exc\n\n        raise Exception(\"Max retries exceeded.\")\n</code></pre> <p>               Bases: <code>Model</code></p> <p>A model interface for Anthropic-like APIs.</p> <p>Attributes:</p> Name Type Description <code>api_base_url</code> <code>str</code> <p>The base URL for the Anthropic API.</p> <code>api_secret_key</code> <code>str</code> <p>The secret key for accessing the Anthropic API.</p> <code>model</code> <code>str</code> <p>The name of the model.</p> <code>instruction_prompt</code> <code>str</code> <p>The default instruction prompt for the model.</p> <code>model_parameters</code> <code>dict</code> <p>Additional parameters specific to the model.</p> <code>completion_parameters</code> <code>dict</code> <p>Parameters for generating completions.</p> <code>retry_on_ratelimit</code> <p>bool = False,</p> <code>cooldown_interval</code> <p>int = 10,</p> <code>max_retries</code> <p>int = 1,</p> <code>client</code> <code>Anthropic</code> <p>An instance of the Anthropic client for API interactions.</p> <p>Methods:</p> Name Description <code>model_name</code> <p>Returns the name of the model.</p> <code>prompt_formatter</code> <p>str) -&gt; list[dict]: Formats the prompt into a list of messages.</p> <code>get_prompt_completion</code> <p>str) -&gt; str: Generates completion for a given prompt.</p> <code>generate_completions</code> <p>TaskMatchGroup, prefer_concurrency: bool = True, n_workers: int = 4) -&gt; TaskMatchGroup: Generates completions for a list of matches.</p> Source code in <code>parsbench/models/anthropic_interface.py</code> <pre><code>class AnthropicModel(Model):\n    \"\"\"\n    A model interface for Anthropic-like APIs.\n\n    Attributes:\n        api_base_url (str): The base URL for the Anthropic API.\n        api_secret_key (str): The secret key for accessing the Anthropic API.\n        model (str): The name of the model.\n        instruction_prompt (str): The default instruction prompt for the model.\n        model_parameters (dict): Additional parameters specific to the model.\n        completion_parameters (dict): Parameters for generating completions.\n        retry_on_ratelimit: bool = False,\n        cooldown_interval: int = 10,\n        max_retries: int = 1,\n        client (Anthropic): An instance of the Anthropic client for API interactions.\n\n    Methods:\n        model_name(self) -&gt; str: Returns the name of the model.\n        prompt_formatter(self, prompt: str) -&gt; list[dict]: Formats the prompt into a list of messages.\n        get_prompt_completion(self, prompt: str) -&gt; str: Generates completion for a given prompt.\n        generate_completions(self, matches: TaskMatchGroup, prefer_concurrency: bool = True, n_workers: int = 4) -&gt; TaskMatchGroup: Generates completions for a list of matches.\n    \"\"\"\n\n    support_concurrency: bool = True\n\n    def __init__(\n        self,\n        api_secret_key: str,\n        model: str,\n        api_base_url: str | None = None,\n        instruction_prompt: str = DEFAULT_INSTRUCTION_PROMPT,\n        model_parameters: dict = None,\n        completion_parameters: dict = None,\n        retry_on_ratelimit: bool = False,\n        cooldown_interval: int = 10,\n        max_retries: int = 1,\n        **kwargs\n    ):\n        self.api_base_url = api_base_url\n        self.api_secret_key = api_secret_key\n        self.model = model\n        self.instruction_prompt = instruction_prompt\n        self.model_parameters = model_parameters or dict()\n        self.completion_parameters = completion_parameters or dict(\n            max_tokens=1024, temperature=0.7\n        )\n        self.retry_on_ratelimit = retry_on_ratelimit\n        self.cooldown_interval = cooldown_interval\n        self.max_retries = max_retries\n\n        self.client = Anthropic(\n            base_url=self.api_base_url,\n            api_key=self.api_secret_key,\n            **self.model_parameters,\n        )\n\n    @property\n    def model_name(self) -&gt; str:\n        return self.model\n\n    def prompt_formatter(self, prompt: str) -&gt; list[dict]:\n        messages = [\n            {\"role\": \"user\", \"content\": prompt},\n        ]\n        return messages\n\n    def get_prompt_completion(self, prompt: str) -&gt; str:\n        messages = self.prompt_formatter(prompt)\n\n        retries = 0\n        while retries &lt; self.max_retries:\n            try:\n                message = self.client.messages.create(\n                    model=self.model,\n                    messages=messages,\n                    system=self.instruction_prompt,\n                    **self.completion_parameters,\n                    stream=False,  # Always override this parameter.\n                )\n                return message.content[0].text\n            except RateLimitError as exc:\n                if self.retry_on_ratelimit:\n                    retries += 1\n                    time.sleep(self.cooldown_interval)\n                else:\n                    raise exc\n\n        raise Exception(\"Max retries exceeded.\")\n</code></pre> <p>               Bases: <code>Model</code></p> <p>A model interface for pre-trained transformer models.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>PreTrainedModel</code> <p>The pre-trained transformer model.</p> <code>tokenizer</code> <code>PreTrainedTokenizer</code> <p>The tokenizer associated with the model.</p> <code>generation_config</code> <code>GenerationConfig</code> <p>The generation configuration for text generation.</p> <code>instruction_prompt</code> <code>str</code> <p>The default instruction prompt for the model.</p> <code>custom_prompt_formatter</code> <code>Callable[[str], str] | None</code> <p>A custom prompt formatter function.</p> <p>Methods:</p> Name Description <code>model_name</code> <p>Returns the base model prefix of the transformer model.</p> <code>prompt_formatter</code> <p>Formats a prompt by combining system instruction and user input. Could be overloaded.</p> <code>completion_formatter</code> <p>Method to format the model completion. Could be overloaded.</p> <code>get_prompt_completion</code> <p>Generates a completion for a given prompt using the model and tokenizer.</p> Source code in <code>parsbench/models/transformers_interface.py</code> <pre><code>class PreTrainedTransformerModel(Model):\n    \"\"\"\n    A model interface for pre-trained transformer models.\n\n    Attributes:\n        model (PreTrainedModel): The pre-trained transformer model.\n        tokenizer (PreTrainedTokenizer): The tokenizer associated with the model.\n        generation_config (GenerationConfig): The generation configuration for text generation.\n        instruction_prompt (str): The default instruction prompt for the model.\n        custom_prompt_formatter (Callable[[str], str] | None): A custom prompt formatter function.\n\n    Methods:\n        model_name: Returns the base model prefix of the transformer model.\n        prompt_formatter: Formats a prompt by combining system instruction and user input. Could be overloaded.\n        completion_formatter: Method to format the model completion. Could be overloaded.\n        get_prompt_completion: Generates a completion for a given prompt using the model and tokenizer.\n    \"\"\"\n\n    support_concurrency: bool = False  # TODO: should support later.\n\n    def __init__(\n        self,\n        model: PreTrainedModel,\n        tokenizer: PreTrainedTokenizer,\n        generation_config: GenerationConfig = DEFAULT_GENERATION_CONFIG,\n        instruction_prompt: str = DEFAULT_INSTRUCTION_PROMPT,\n        custom_prompt_formatter: Callable[[str], str] | None = None,\n    ):\n        self.model = model\n        self.tokenizer = tokenizer\n        self.generation_config = generation_config\n        self.instruction_prompt = instruction_prompt\n        self.custom_prompt_formatter = custom_prompt_formatter\n\n    @property\n    def model_name(self) -&gt; str:\n        return self.model.config.name_or_path or \"model\"\n\n    def prompt_formatter(self, prompt: str) -&gt; str:\n        messages = [\n            {\"role\": \"system\", \"content\": self.instruction_prompt},\n            {\"role\": \"user\", \"content\": prompt},\n        ]\n        text = self.tokenizer.apply_chat_template(\n            messages, tokenize=False, add_generation_prompt=True\n        )\n        return text\n\n    def get_prompt_completion(self, prompt: str) -&gt; str:\n        if self.custom_prompt_formatter:\n            input_text = self.custom_prompt_formatter(prompt)\n        else:\n            input_text = self.prompt_formatter(prompt)\n\n        model_inputs = self.tokenizer([input_text], return_tensors=\"pt\").to(\n            self.model.device\n        )\n\n        generated_ids = self.model.generate(\n            model_inputs.input_ids,\n            generation_config=self.generation_config,\n            attention_mask=model_inputs.attention_mask,\n        )\n        generated_ids = [\n            output_ids[len(input_ids) :]\n            for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n        ]\n\n        response = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[\n            0\n        ]\n        return response\n</code></pre>"},{"location":"code_reference/scores/","title":"Scores","text":"<p>Class representing a Scorer object.</p> <p>Attributes:</p> Name Type Description <code>func</code> <code>Callable[[str, str], float]</code> <p>The scoring function to be wrapped.</p> <p>Methods:</p> Name Description <code>measure</code> <p>str, target: str) -&gt; float: Calculates the score between the completion and target strings using the wrapped scoring function.</p> <code>name</code> <p>Returns the name of the wrapped scoring function with underscores replaced by spaces and title-cased.</p> Source code in <code>parsbench/scores/base.py</code> <pre><code>class Scorer:\n    \"\"\"\n    Class representing a Scorer object.\n\n    Attributes:\n        func (Callable[[str, str], float]): The scoring function to be wrapped.\n\n    Methods:\n        measure(completion: str, target: str) -&gt; float:\n            Calculates the score between the completion and target strings using the wrapped scoring function.\n\n        name() -&gt; str:\n            Returns the name of the wrapped scoring function with underscores replaced by spaces and title-cased.\n    \"\"\"\n\n    def __init__(self, func: Callable[[str, str], float]):\n        self.func = func\n\n    def measure(self, completion: str, target: str) -&gt; float:\n        return self.func(completion, target)\n\n    @property\n    def name(self) -&gt; str:\n        return self.func.__name__.replace(\"_\", \" \").title()\n</code></pre> <p>Wraps a scorer function inside the Scorer class.</p> Source code in <code>parsbench/scores/base.py</code> <pre><code>def wrap_scorer(func):\n    \"\"\"Wraps a scorer function inside the Scorer class.\"\"\"\n    return Scorer(func)\n</code></pre> Source code in <code>parsbench/scores/common.py</code> <pre><code>@wrap_scorer\ndef exact_match(completion: str, target: str) -&gt; int:\n    return int(completion == target)\n</code></pre> Source code in <code>parsbench/scores/machine_translation.py</code> <pre><code>@wrap_scorer\ndef english_sentence_bleu(completion: str, target: str) -&gt; float:\n    nltk.download(\"punkt\", quiet=True)\n\n    reference_translation = [nltk.word_tokenize(target)]\n    model_translation = nltk.word_tokenize(completion)\n    bleu_score = nltk.translate.bleu(\n        reference_translation, model_translation, weights=(1,)\n    )\n    return bleu_score\n</code></pre> Source code in <code>parsbench/scores/machine_translation.py</code> <pre><code>@wrap_scorer\ndef persian_sentence_bleu(completion: str, target: str) -&gt; float:\n    reference_translation = [hazm.word_tokenize(target)]\n    model_translation = hazm.word_tokenize(completion)\n    bleu_score = nltk.translate.bleu(\n        reference_translation, model_translation, weights=(1,)\n    )\n    return bleu_score\n</code></pre> Source code in <code>parsbench/scores/summarization.py</code> <pre><code>@wrap_scorer\ndef english_rouge(completion: str, target: str) -&gt; float:\n    nltk.download(\"punkt\", quiet=True)\n\n    tokenizer = nltk.tokenize.NLTKWordTokenizer()\n\n    scorer = rouge_scorer.RougeScorer([\"rouge1\"], tokenizer=tokenizer)\n    scores = scorer.score(target, completion)\n    return scores[\"rouge1\"].fmeasure\n</code></pre> Source code in <code>parsbench/scores/summarization.py</code> <pre><code>@wrap_scorer\ndef persian_rouge(completion: str, target: str) -&gt; float:\n    tokenizer = hazm.WordTokenizer()\n\n    scorer = rouge_scorer.RougeScorer([\"rouge1\"], tokenizer=tokenizer)\n    scores = scorer.score(target, completion)\n    return scores[\"rouge1\"].fmeasure\n</code></pre>"},{"location":"code_reference/tasks/","title":"Tasks","text":"<p>               Bases: <code>TaskMatchGenerator</code>, <code>TaskScorer</code></p> <p>Task class represents a task that combines functionality from TaskMatchGenerator and TaskScorer.</p> <p>Attributes:</p> Name Type Description <code>task_name</code> <code>str</code> <p>The name of the task.</p> <code>task_category</code> <code>TaskCategory</code> <p>The category of the task.</p> <p>Methods:</p> Name Description <code>evaluate</code> <p>Method to evaluate the task by generating matches, scoring them, and saving the results.</p> Source code in <code>parsbench/tasks/base/task.py</code> <pre><code>class Task(TaskMatchGenerator, TaskScorer, metaclass=ABCMeta):\n    \"\"\"\n    Task class represents a task that combines functionality from TaskMatchGenerator and TaskScorer.\n\n    Attributes:\n        task_name (str): The name of the task.\n        task_category (TaskCategory): The category of the task.\n\n    Methods:\n        evaluate: Method to evaluate the task by generating matches, scoring them, and saving the results.\n    \"\"\"\n\n    task_name: str\n    task_category: TaskCategory\n\n    def evaluate(\n        self,\n        model: Model,\n        prompt_lang: str = \"fa\",\n        prompt_shots: list[int] = None,\n        n_first: int = 200,\n        sub_tasks: list[str] | None = None,\n        save_matches: bool = False,\n        save_evaluation: bool = False,\n        output_path: str = None,\n        skip_existing_matches: bool = False,\n        prefer_concurrency: bool = True,\n        n_workers: int = 4,\n    ) -&gt; list[EvaluationResult]:\n        \"\"\"\n        Method to evaluate the task by generating matches, scoring them, and saving the results.\n\n        Parameters:\n            model (Model): The model to be evaluated.\n            prompt_lang (str, optional): The language of the prompt (default is \"fa\").\n            prompt_shots (list[int], optional): The list of prompt shots to evaluate (default is None).\n            n_first (int, optional): The number of initial prompts to consider (default is 200).\n            sub_tasks (list[str], optional): The list of sub-tasks to evaluate (default is None).\n            save_matches (bool, optional): Flag to save the generated matches (default is False).\n            save_evaluation (bool, optional): Flag to save the evaluation results (default is False).\n            output_path (str, optional): The output path to save the matches and evaluation results.\n            skip_existing_matches (bool, optional): Flag to skip already generated matches in the output path (default is False).\n            prefer_concurrency (bool, optional): The flag to use concurrent processing if the model and task support that (default is True).\n            n_workers (int, optional): The number of workers for concurrent processing (default is 4).\n\n        Returns:\n            list[EvaluationResult]: A list of EvaluationResult objects representing the evaluation results.\n\n        Raises:\n            Exception: If output_path is not provided when saving matches or evaluation.\n            Exception: If output_path is not provided when skipping existing matches.\n            Exception: If sub tasks are not defined or if invalid sub tasks are provided.\n\n        \"\"\"\n        if (save_matches or save_evaluation) and not output_path:\n            raise Exception(\n                \"You should set the output path to save matches/evaluation.\"\n            )\n\n        if skip_existing_matches and not output_path:\n            raise Exception(\n                \"Cannot find already generated matches when output_path is not set.\"\n            )\n\n        task_path = None\n        if output_path:\n            task_path = get_task_path(output_path, model.model_name, self.task_name)\n\n        prompt_shots = [0] if prompt_shots is None else prompt_shots\n\n        if sub_tasks:\n            if not self.sub_tasks:\n                raise Exception(\"Sub tasks are not defined.\")\n\n            invalid_sub_tasks = set(sub_tasks) - set(self.sub_tasks)\n            if invalid_sub_tasks:\n                raise Exception(f\"Sub tasks {invalid_sub_tasks} are not defined.\")\n\n        sub_tasks = sub_tasks or self._selected_sub_tasks or self.sub_tasks\n\n        evaluation_results: list[EvaluationResult] = []\n\n        for sub_task in sub_tasks or [None]:\n            match_groups: list[TaskMatchGroup] = []\n\n            for shots in prompt_shots:\n                if skip_existing_matches and check_task_matches_exists(\n                    task_path, shots, sub_task=sub_task\n                ):\n                    match_group = TaskMatchGroup.from_file(\n                        task_path, shots, sub_task=sub_task\n                    )\n                    match_group._loaded_locally = True\n                else:\n                    match_group = self.generate_matches(\n                        prompt_lang,\n                        n_shots=shots,\n                        n_first=n_first,\n                        sub_task=sub_task,\n                    )\n                match_groups.append(match_group)\n\n            for match_group in match_groups:\n                eval_desc = f\"{match_group.n_shots}-shot\"\n                if sub_task:\n                    eval_desc = f\"sub task '{sub_task}' with \" + eval_desc\n                desc = f\"Evaluating {eval_desc} prompt:\"\n                print(desc)\n\n                is_loaded_locally = getattr(match_group, \"_loaded_locally\", False)\n\n                if is_loaded_locally:\n                    total_skipped = sum(m.completion is not None for m in match_group)\n                    print(\n                        f\"{total_skipped} of {len(match_group)} match completions will be loaded from local.\"\n                    )\n\n                try:\n                    model.generate_completions(\n                        match_group,\n                        prefer_concurrency=prefer_concurrency,\n                        skip_existing=is_loaded_locally,\n                        n_workers=n_workers,\n                    )\n                    self.score_matches(match_group)\n                finally:\n                    if save_matches:\n                        match_group.save(task_path, sub_task=sub_task)\n\n            evaluation_result = EvaluationResult(\n                model_name=model.model_name,\n                task_name=self.task_name,\n                task_category=self.task_category,\n                score_name=self.score_name,\n                sub_task=sub_task,\n                prompt_shot_results=[\n                    PromptShotEvaluationResult(\n                        n_shots=m.n_shots,\n                        score=self.get_overall_score(m),\n                    )\n                    for m in match_groups\n                ],\n            )\n            evaluation_results.append(evaluation_result)\n\n            if save_evaluation:\n                evaluation_result.save(task_path)\n\n        return evaluation_results\n\n    def __enter__(self) -&gt; \"Task\":\n        self.load_data()\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        self._data = None\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>An abstract base class for defining data loaders.</p> <p>Attributes:</p> Name Type Description <code>data_path</code> <code>str</code> <p>The path to the data source.</p> <p>Methods:</p> Name Description <code>load</code> <p>Abstract method to be implemented by subclasses for loading data.</p> Source code in <code>parsbench/tasks/base/data_loader.py</code> <pre><code>class DataLoader(ABC):\n    \"\"\"\n    An abstract base class for defining data loaders.\n\n    Attributes:\n        data_path (str): The path to the data source.\n\n    Methods:\n        load(self) -&gt; list[dict]: Abstract method to be implemented by subclasses for loading data.\n    \"\"\"\n\n    def __init__(self, data_path: str, **kwargs) -&gt; None:\n        self.data_path = data_path\n\n    @abstractmethod\n    def load(self) -&gt; list[dict]:\n        pass\n</code></pre> <p>               Bases: <code>DataLoader</code></p> <p>A data loader class for loading JSON line data from either a local file or a URL.</p> <p>Attributes:</p> Name Type Description <code>data_path</code> <code>str</code> <p>The path to the JSON line data source.</p> <p>Methods:</p> Name Description <code>load</code> <p>Loads the JSON line data from the specified source.</p> Source code in <code>parsbench/tasks/base/data_loader.py</code> <pre><code>class JSONLineDataLoader(DataLoader):\n    \"\"\"\n    A data loader class for loading JSON line data from either a local file or a URL.\n\n    Attributes:\n        data_path (str): The path to the JSON line data source.\n\n    Methods:\n        load(self) -&gt; list[dict]: Loads the JSON line data from the specified source.\n    \"\"\"\n\n    def load(self) -&gt; list[dict]:\n        content = _fetch_text_file(self.data_path)\n\n        reader = jsonlines.Reader(content.split(\"\\n\"))\n        return list(reader.iter(type=dict, skip_invalid=True, skip_empty=True))\n</code></pre> <p>               Bases: <code>DataLoader</code></p> <p>A data loader class for loading datasets using the Hugging Face library.</p> <p>Attributes:</p> Name Type Description <code>data_path</code> <code>str</code> <p>The path to the data source.</p> <code>split</code> <code>str | None</code> <p>The split of the dataset to load.</p> <p>Methods:</p> Name Description <code>load</code> <p>Loads the dataset from the specified data path and split.</p> <code>with_filter</code> <p>Callable[..., bool]) -&gt; \"HuggingFaceDataLoader\": Adds a filter function to apply when loading the dataset.</p> Source code in <code>parsbench/tasks/base/data_loader.py</code> <pre><code>class HuggingFaceDataLoader(DataLoader):\n    \"\"\"\n    A data loader class for loading datasets using the Hugging Face library.\n\n    Attributes:\n        data_path (str): The path to the data source.\n        split (str | None): The split of the dataset to load.\n\n    Methods:\n        load(self) -&gt; list[dict]: Loads the dataset from the specified data path and split.\n        with_filter(self, func: Callable[..., bool]) -&gt; \"HuggingFaceDataLoader\": Adds a filter function to apply when loading the dataset.\n    \"\"\"\n\n    def __init__(\n        self,\n        data_path: str,\n        split: str | None = None,\n        **optional_parameters: dict[str, Any],\n    ) -&gt; None:\n        super().__init__(data_path)\n        self.split = split\n        self.optional_parameters = optional_parameters\n        self._filters = []\n\n    def load(self) -&gt; list[dict]:\n        dataset = datasets.load_dataset(\n            self.data_path, split=self.split, **self.optional_parameters\n        )\n        if len(self._filters):\n            for filter_ in self._filters:\n                dataset = dataset.filter(filter_)\n        return dataset.to_list()\n\n    def with_filter(self, func: Callable[..., bool]) -&gt; \"HuggingFaceDataLoader\":\n        self._filters.append(func)\n        return self\n</code></pre> <p>               Bases: <code>DataLoader</code></p> <p>A data loader class for loading CSV line data from either a local file or a URL.</p> <p>Attributes:</p> Name Type Description <code>data_path</code> <code>str</code> <p>The path to the CSV line data source.</p> <p>Methods:</p> Name Description <code>load</code> <p>Loads the CSV line data from the specified source.</p> Source code in <code>parsbench/tasks/base/data_loader.py</code> <pre><code>class CSVDataLoader(DataLoader):\n    \"\"\"\n    A data loader class for loading CSV line data from either a local file or a URL.\n\n    Attributes:\n        data_path (str): The path to the CSV line data source.\n\n    Methods:\n        load(self) -&gt; list[dict]: Loads the CSV line data from the specified source.\n    \"\"\"\n\n    def __init__(self, data_path: str, csv_arguments: dict | None = None, **kwargs):\n        super().__init__(data_path)\n        self.csv_arguments = csv_arguments or {}\n\n    def load(self) -&gt; list[dict]:\n        content = _fetch_text_file(self.data_path)\n\n        csv_reader = csv.DictReader(content.split(\"\\n\"), **self.csv_arguments)\n        return list(csv_reader)\n</code></pre> <p>A class representing a prompt template.</p> <p>Attributes:</p> Name Type Description <code>language_templates</code> <code>dict[str, str]</code> <p>A dictionary mapping language codes to prompt templates.</p> <code>prompt_variables_mapping</code> <code>dict[str, str]</code> <p>A dictionary mapping prompt variable names to corresponding data keys.</p> <code>target_variables_mapping</code> <code>dict[str, str]</code> <p>A dictionary mapping target variable names to corresponding data keys.</p> <code>prompt_shot_templates</code> <code>dict[str, str] | None</code> <p>A dictionary mapping prompt shot templates to language codes, or None if not provided.</p> <code>prompt_shot_examples</code> <code>dict[str, dict[int, str]] | None</code> <p>A dictionary mapping prompt shot examples to language codes and shot numbers, or None if not provided.</p> Source code in <code>parsbench/tasks/base/prompt_template.py</code> <pre><code>class PromptTemplate:\n    \"\"\"\n    A class representing a prompt template.\n\n    Attributes:\n        language_templates (dict[str, str]): A dictionary mapping language codes to prompt templates.\n        prompt_variables_mapping (dict[str, str]): A dictionary mapping prompt variable names to corresponding data keys.\n        target_variables_mapping (dict[str, str]): A dictionary mapping target variable names to corresponding data keys.\n        prompt_shot_templates (dict[str, str] | None): A dictionary mapping prompt shot templates to language codes, or None if not provided.\n        prompt_shot_examples (dict[str, dict[int, str]] | None): A dictionary mapping prompt shot examples to language codes and shot numbers, or None if not provided.\n    \"\"\"\n\n    def __init__(\n        self,\n        language_templates: dict[str, str],\n        prompt_variables_mapping: dict[str, str],\n        target_variables_mapping: dict[str, str],\n        prompt_shot_templates: dict[str, str] | None = None,\n        prompt_shot_examples: dict[str, dict[int, str]] | None = None,\n    ):\n        self.language_templates = language_templates\n        self.prompt_variables_mapping = prompt_variables_mapping\n        self.target_variables_mapping = target_variables_mapping\n\n        if prompt_shot_templates is not None and prompt_shot_examples is not None:\n            raise ValueError(\"Cannot provide both prompt shot templates and examples\")\n\n        if prompt_shot_templates is None and prompt_shot_examples is None:\n            raise ValueError(\"Must provide either prompt shot templates or examples\")\n\n        self.prompt_shot_templates = prompt_shot_templates\n        self.prompt_shot_examples = prompt_shot_examples\n\n    def get_prompt(\n        self,\n        prompt_lang: str,\n        data: dict,\n        n_shots: int = 0,\n        sample_data: list[dict] | None = None,\n    ):\n        prompt_template = self.language_templates.get(prompt_lang, None)\n        if not prompt_template:\n            raise RuntimeError(\n                f\"There is no prompt template for language {prompt_lang}.\"\n            )\n\n        if n_shots &gt; 0:\n            if sample_data:\n                example_text = self._gen_example_text(prompt_lang, n_shots, sample_data)\n            else:\n                example_text = self._get_static_example_text(prompt_lang, n_shots)\n        else:\n            example_text = \"\"\n\n        prompt = prompt_template.format(\n            example_shots=example_text, **self.get_prompt_variables(data)\n        )\n        prompt = prompt.replace(\"\\n\\n\\n\", \"\\n\")\n\n        return prompt\n\n    def get_prompt_variables(self, data: dict) -&gt; dict:\n        mapped_data = {}\n        for pk, dk in self.prompt_variables_mapping.items():\n            if isinstance(dk, ConstantPromptVariable):\n                mapped_data[pk] = dk.value\n            else:\n                if dk not in data:\n                    raise ValueError(f\"Key {dk} not in data.\")\n                mapped_data[pk] = data[dk]\n        return mapped_data\n\n    def get_target_variables(self, data: dict) -&gt; dict:\n        mapped_data = {}\n        for tk, dk in self.target_variables_mapping.items():\n            if dk not in data:\n                raise ValueError(f\"Key {dk} not in data.\")\n            mapped_data[tk] = data[dk]\n        return mapped_data\n\n    def _get_static_example_text(self, prompt_lang: str, n_shots: int) -&gt; str:\n        shot_examples = self.prompt_shot_examples.get(prompt_lang, None)\n        if not shot_examples:\n            raise RuntimeError(f\"There is no shot example for language {prompt_lang}.\")\n\n        example_text = shot_examples.get(n_shots, \"\")\n        if not example_text:\n            raise RuntimeError(\n                f\"There is no {n_shots}-shot example for langauge {prompt_lang}. \"\n                f\"You can only use {', '.join(map(str, shot_examples.keys()))} shot examples.\"\n            )\n\n        return example_text\n\n    def _gen_example_text(\n        self, prompt_lang: str, n_shots: int, sample_data: list[dict]\n    ) -&gt; str:\n        if len(sample_data) != n_shots:\n            raise RuntimeError(\n                f\"The number of samples ({len(sample_data)}) is not equal to the number of shots ({n_shots}).\"\n            )\n\n        if shot_template := self.prompt_shot_templates.get(prompt_lang):\n            example_text = \"\\n\".join(\n                shot_template.format(\n                    **self.get_prompt_variables(sample),\n                    **self.get_target_variables(sample),\n                )\n                for sample in sample_data\n            )\n        else:\n            sample_variables = [\n                {\n                    **self.get_prompt_variables(sample),\n                    **self.get_target_variables(sample),\n                }\n                for sample in sample_data\n            ]\n            example_text = \"\\n\".join(\n                \"\\n\".join(f\"{k.capitalize()}:\\n{v}\" for k, v in variables)\n                for variables in sample_variables\n            )\n\n        return example_text\n\n    @property\n    def has_shot_templates(self) -&gt; bool:\n        return bool(self.prompt_shot_templates)\n\n    @property\n    def has_shot_examples(self) -&gt; bool:\n        return bool(self.prompt_shot_examples)\n</code></pre> <p>               Bases: <code>Mapping</code></p> <p>A class representing lazy loading of templates.</p> <p>Inherits from Mapping.</p> <p>Attributes:</p> Name Type Description <code>template_paths</code> <code>dict[str, str]</code> <p>A dictionary mapping template keys to file paths.</p> Source code in <code>parsbench/tasks/base/prompt_template.py</code> <pre><code>class LazyLoadTemplates(Mapping):\n    \"\"\"\n    A class representing lazy loading of templates.\n\n    Inherits from Mapping.\n\n    Attributes:\n        template_paths (dict[str, str]): A dictionary mapping template keys to file paths.\n    \"\"\"\n\n    def __init__(self, template_paths: dict[str, str] | None = None, **kwargs):\n        super().__init__()\n        self.template_paths = template_paths or kwargs or {}\n        self._contents: dict[str, str | None] = {\n            key: None for key in self.template_paths\n        }\n\n    def _load_content(self, key):\n        if key in self.template_paths:\n            with open(self.template_paths[key], \"r\") as file:\n                self._contents[key] = file.read()\n        else:\n            raise KeyError(f\"Key '{key}' not found in template_paths\")\n\n    def __getitem__(self, key) -&gt; str:\n        if key not in self._contents:\n            raise KeyError(f\"Key '{key}' not found\")\n        if self._contents[key] is None:\n            self._load_content(key)\n        return self._contents[key]\n\n    def __getattr__(self, key) -&gt; str:\n        try:\n            return self.__getitem__(key)\n        except KeyError:\n            raise AttributeError(f\"Attribute '{key}' not found\")\n\n    def __iter__(self):\n        return iter(self.template_paths)\n\n    def __len__(self):\n        return len(self.template_paths)\n</code></pre> <p>A data class representing the evaluation result for a prompt shot, including the number of shots and the corresponding score.</p> <p>Attributes:</p> Name Type Description <code>n_shots</code> <code>int</code> <p>The number of shots for the evaluation.</p> <code>score</code> <code>float</code> <p>The score obtained for the prompt shot evaluation.</p> Source code in <code>parsbench/tasks/base/evaluation_result.py</code> <pre><code>@dataclass\nclass PromptShotEvaluationResult:\n    \"\"\"\n    A data class representing the evaluation result for a prompt shot, including the number of shots and the corresponding score.\n\n    Attributes:\n        n_shots (int): The number of shots for the evaluation.\n        score (float): The score obtained for the prompt shot evaluation.\n    \"\"\"\n\n    n_shots: int\n    score: float\n\n    @classmethod\n    def from_dict(cls, data: dict) -&gt; \"PromptShotEvaluationResult\":\n        return cls(**data)\n\n    def to_dict(self) -&gt; dict:\n        return asdict(self)\n\n    def to_pandas(self) -&gt; pd.DataFrame:\n        return pd.DataFrame([self])\n\n    def __str__(self) -&gt; str:\n        return f\"{self.n_shots}-shot score: {self.score:.4f}\"\n</code></pre> <p>A data class representing the evaluation result for a model on a specific task, including the model name, task name, task category, score name, prompt shot results, and optional sub-task.</p> <p>Attributes:</p> Name Type Description <code>model_name</code> <code>str</code> <p>The name of the model being evaluated.</p> <code>task_name</code> <code>str</code> <p>The name of the task for which the model is being evaluated.</p> <code>task_category</code> <code>TaskCategory</code> <p>The category of the task (e.g., CLASSIC, REASONING, MATH, KNOWLEDGE).</p> <code>score_name</code> <code>str</code> <p>The name of the score obtained for the evaluation.</p> <code>prompt_shot_results</code> <code>list[PromptShotEvaluationResult]</code> <p>A list of PromptShotEvaluationResult objects representing the evaluation results for prompt shots.</p> <code>sub_task</code> <code>str</code> <p>The name of the sub-task being evaluated, if applicable.</p> Source code in <code>parsbench/tasks/base/evaluation_result.py</code> <pre><code>@dataclass\nclass EvaluationResult:\n    \"\"\"\n    A data class representing the evaluation result for a model on a specific task, including the model name, task name, task category, score name, prompt shot results, and optional sub-task.\n\n    Attributes:\n        model_name (str): The name of the model being evaluated.\n        task_name (str): The name of the task for which the model is being evaluated.\n        task_category (TaskCategory): The category of the task (e.g., CLASSIC, REASONING, MATH, KNOWLEDGE).\n        score_name (str): The name of the score obtained for the evaluation.\n        prompt_shot_results (list[PromptShotEvaluationResult]): A list of PromptShotEvaluationResult objects representing the evaluation results for prompt shots.\n        sub_task (str, optional): The name of the sub-task being evaluated, if applicable.\n    \"\"\"\n\n    model_name: str\n    task_name: str\n    task_category: TaskCategory\n    score_name: str\n    prompt_shot_results: list[PromptShotEvaluationResult]\n    sub_task: str | None = None\n\n    @classmethod\n    def from_file(cls, path: str) -&gt; \"EvaluationResult\":\n        with jsonlines.open(path, \"r\") as reader:\n            data = reader.read(type=dict)\n            return cls.from_dict(data)\n\n    @classmethod\n    def from_dict(cls, data: dict) -&gt; \"EvaluationResult\":\n        prompt_shot_results = [\n            PromptShotEvaluationResult.from_dict(psr)\n            for psr in data.pop(\"prompt_shot_results\")\n        ]\n        data[\"task_category\"] = TaskCategory[data[\"task_category\"].upper()]\n        return cls(**data, prompt_shot_results=prompt_shot_results)\n\n    def to_dict(self) -&gt; dict:\n        return {\n            **asdict(self),\n            \"prompt_shot_results\": [e.to_dict() for e in self.prompt_shot_results],\n        }\n\n    def to_pandas(self) -&gt; pd.DataFrame:\n        data = [\n            {\n                \"model_name\": self.model_name,\n                \"task_name\": self.task_name,\n                \"task_category\": self.task_category.value,\n                \"sub_task\": self.sub_task,\n                \"n_shots\": psr.n_shots,\n                \"score_name\": self.score_name,\n                \"score\": psr.score,\n            }\n            for psr in self.prompt_shot_results\n        ]\n        return pd.DataFrame(data)\n\n    def save(self, path: str):\n        file_name = (\n            f\"evaluation_{self.sub_task}.jsonl\" if self.sub_task else \"evaluation.jsonl\"\n        )\n        task_path = path / file_name\n\n        with jsonlines.open(task_path, \"w\") as writer:\n            writer.write(self.to_dict())\n\n    def __str__(self) -&gt; str:\n        text = f\"Model: {self.model_name}\\nTask: {self.task_name}\"\n\n        if self.sub_task:\n            text += f\" ({self.sub_task})\"\n\n        text += \"\\nScore:\\n\"\n\n        for psr in self.prompt_shot_results:\n            text += f\" - {psr.n_shots}-shot prompt: {psr.score:.4f}\\n\"\n        return text.strip(\"\\n\")\n\n    @property\n    def average_score(self) -&gt; float:\n        return sum([psr.score for psr in self.prompt_shot_results]) / len(\n            self.prompt_shot_results\n        )\n\n    @property\n    def max_score(self) -&gt; float:\n        return max([psr.score for psr in self.prompt_shot_results])\n</code></pre> Source code in <code>parsbench/tasks/base/task_match.py</code> <pre><code>@dataclass\nclass TaskMatch:\n    id: int\n    prompt: str\n    target: str\n    completion: str | None = None\n    formatted_completion: str | None = None\n    score: int | None = None\n\n    @classmethod\n    def from_dict(cls, data: dict) -&gt; \"TaskMatch\":\n        return cls(**data)\n\n    def format_completion(self, formatter: Callable[[str], str]):\n        self.formatted_completion = formatter(self.completion)\n\n    def format_prompt(self, formatter: Callable[[str], str]):\n        self.prompt = formatter(self.prompt)\n\n    def format_target(self, formatter: Callable[[str], str]):\n        self.target = formatter(self.target)\n\n    def to_dict(self) -&gt; dict:\n        return asdict(self)\n\n    def to_pandas(self) -&gt; pd.DataFrame:\n        return pd.DataFrame([self])\n\n    @property\n    def cleaned_completion(self) -&gt; str | None:\n        return self.formatted_completion or self.completion\n</code></pre> Source code in <code>parsbench/tasks/base/task_match.py</code> <pre><code>@dataclass\nclass TaskMatchGroup:\n    n_shots: int\n    matches: list[TaskMatch]\n\n    def __iter__(self):\n        yield from iter(self.matches)\n\n    def __len__(self) -&gt; int:\n        return len(self.matches)\n\n    @classmethod\n    def from_file(\n        cls, path: str, n_shots: int, sub_task: str | None\n    ) -&gt; \"TaskMatchGroup\":\n        if sub_task:\n            matches_path = path / f\"matches_{sub_task}_{n_shots}_shot.jsonl\"\n        else:\n            matches_path = path / f\"matches_{n_shots}_shot.jsonl\"\n\n        with jsonlines.open(matches_path, \"r\") as reader:\n            matches: list[TaskMatch] = []\n            for row in reader.iter(type=dict, skip_invalid=True):\n                matches.append(TaskMatch.from_dict(row))\n\n        return cls(n_shots=n_shots, matches=matches)\n\n    @classmethod\n    def from_dict(cls, data: dict) -&gt; \"TaskMatchGroup\":\n        matches = [TaskMatch.from_dict(m) for m in data.pop(\"matches\")]\n        return cls(**data, matches=matches)\n\n    def format_completions(self, formatter: Callable[[str], str]):\n        for m in self.matches:\n            m.format_completion(formatter)\n\n    def format_prompts(self, formatter: Callable[[str], str]):\n        for m in self.matches:\n            m.format_prompt(formatter)\n\n    def format_targets(self, formatter: Callable[[str], str]):\n        for m in self.matches:\n            m.format_target(formatter)\n\n    def to_dict(self) -&gt; dict:\n        return {\n            **asdict(self),\n            \"matches\": [match.to_dict() for match in self.matches],\n        }\n\n    def to_pandas(self) -&gt; pd.DataFrame:\n        df = pd.DataFrame(\n            [\n                {\n                    **asdict(match),\n                    \"n_shots\": self.n_shots,\n                }\n                for match in self.matches\n            ]\n        )\n        return df\n\n    def save(self, path: str, sub_task: str | None):\n        if sub_task:\n            matches_path = path / f\"matches_{sub_task}_{self.n_shots}_shot.jsonl\"\n        else:\n            matches_path = path / f\"matches_{self.n_shots}_shot.jsonl\"\n\n        with jsonlines.open(matches_path, \"w\") as writer:\n            writer.write_all(self.to_dict()[\"matches\"])\n\n    @property\n    def prompts(self) -&gt; list[str]:\n        return [m.prompt for m in self.matches]\n\n    @property\n    def targets(self) -&gt; list[str]:\n        return [m.target for m in self.matches]\n\n    @property\n    def completions(self) -&gt; list[str | None]:\n        return [m.completion for m in self.matches]\n\n    @property\n    def scores(self) -&gt; list[int | None]:\n        return [m.score for m in self.matches]\n</code></pre> <p>Load all tasks from the 'parsbench.tasks' package and return a list of Task objects.</p> <p>Returns:</p> Type Description <code>list[Task]</code> <p>list[Task]: A list of Task objects representing all tasks found in the 'parsbench.tasks' package.</p> Source code in <code>parsbench/tasks/utils.py</code> <pre><code>def load_all_tasks() -&gt; list[Task]:\n    \"\"\"\n    Load all tasks from the 'parsbench.tasks' package and return a list of Task objects.\n\n    Returns:\n        list[Task]: A list of Task objects representing all tasks found in the 'parsbench.tasks' package.\n\n    \"\"\"\n    tasks: list[Task] = []\n\n    module = importlib.import_module(\"parsbench.tasks\")\n    for attr_name in dir(module):\n        attr = getattr(module, attr_name)\n        if isinstance(attr, type) and issubclass(attr, Task) and attr is not Task:\n            tasks.append(attr)\n\n    return tasks\n</code></pre>"},{"location":"code_reference/tasks/#parsbench.tasks.base.Task.evaluate","title":"<code>evaluate(model, prompt_lang='fa', prompt_shots=None, n_first=200, sub_tasks=None, save_matches=False, save_evaluation=False, output_path=None, skip_existing_matches=False, prefer_concurrency=True, n_workers=4)</code>","text":"<p>Method to evaluate the task by generating matches, scoring them, and saving the results.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>The model to be evaluated.</p> required <code>prompt_lang</code> <code>str</code> <p>The language of the prompt (default is \"fa\").</p> <code>'fa'</code> <code>prompt_shots</code> <code>list[int]</code> <p>The list of prompt shots to evaluate (default is None).</p> <code>None</code> <code>n_first</code> <code>int</code> <p>The number of initial prompts to consider (default is 200).</p> <code>200</code> <code>sub_tasks</code> <code>list[str]</code> <p>The list of sub-tasks to evaluate (default is None).</p> <code>None</code> <code>save_matches</code> <code>bool</code> <p>Flag to save the generated matches (default is False).</p> <code>False</code> <code>save_evaluation</code> <code>bool</code> <p>Flag to save the evaluation results (default is False).</p> <code>False</code> <code>output_path</code> <code>str</code> <p>The output path to save the matches and evaluation results.</p> <code>None</code> <code>skip_existing_matches</code> <code>bool</code> <p>Flag to skip already generated matches in the output path (default is False).</p> <code>False</code> <code>prefer_concurrency</code> <code>bool</code> <p>The flag to use concurrent processing if the model and task support that (default is True).</p> <code>True</code> <code>n_workers</code> <code>int</code> <p>The number of workers for concurrent processing (default is 4).</p> <code>4</code> <p>Returns:</p> Type Description <code>list[EvaluationResult]</code> <p>list[EvaluationResult]: A list of EvaluationResult objects representing the evaluation results.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If output_path is not provided when saving matches or evaluation.</p> <code>Exception</code> <p>If output_path is not provided when skipping existing matches.</p> <code>Exception</code> <p>If sub tasks are not defined or if invalid sub tasks are provided.</p> Source code in <code>parsbench/tasks/base/task.py</code> <pre><code>def evaluate(\n    self,\n    model: Model,\n    prompt_lang: str = \"fa\",\n    prompt_shots: list[int] = None,\n    n_first: int = 200,\n    sub_tasks: list[str] | None = None,\n    save_matches: bool = False,\n    save_evaluation: bool = False,\n    output_path: str = None,\n    skip_existing_matches: bool = False,\n    prefer_concurrency: bool = True,\n    n_workers: int = 4,\n) -&gt; list[EvaluationResult]:\n    \"\"\"\n    Method to evaluate the task by generating matches, scoring them, and saving the results.\n\n    Parameters:\n        model (Model): The model to be evaluated.\n        prompt_lang (str, optional): The language of the prompt (default is \"fa\").\n        prompt_shots (list[int], optional): The list of prompt shots to evaluate (default is None).\n        n_first (int, optional): The number of initial prompts to consider (default is 200).\n        sub_tasks (list[str], optional): The list of sub-tasks to evaluate (default is None).\n        save_matches (bool, optional): Flag to save the generated matches (default is False).\n        save_evaluation (bool, optional): Flag to save the evaluation results (default is False).\n        output_path (str, optional): The output path to save the matches and evaluation results.\n        skip_existing_matches (bool, optional): Flag to skip already generated matches in the output path (default is False).\n        prefer_concurrency (bool, optional): The flag to use concurrent processing if the model and task support that (default is True).\n        n_workers (int, optional): The number of workers for concurrent processing (default is 4).\n\n    Returns:\n        list[EvaluationResult]: A list of EvaluationResult objects representing the evaluation results.\n\n    Raises:\n        Exception: If output_path is not provided when saving matches or evaluation.\n        Exception: If output_path is not provided when skipping existing matches.\n        Exception: If sub tasks are not defined or if invalid sub tasks are provided.\n\n    \"\"\"\n    if (save_matches or save_evaluation) and not output_path:\n        raise Exception(\n            \"You should set the output path to save matches/evaluation.\"\n        )\n\n    if skip_existing_matches and not output_path:\n        raise Exception(\n            \"Cannot find already generated matches when output_path is not set.\"\n        )\n\n    task_path = None\n    if output_path:\n        task_path = get_task_path(output_path, model.model_name, self.task_name)\n\n    prompt_shots = [0] if prompt_shots is None else prompt_shots\n\n    if sub_tasks:\n        if not self.sub_tasks:\n            raise Exception(\"Sub tasks are not defined.\")\n\n        invalid_sub_tasks = set(sub_tasks) - set(self.sub_tasks)\n        if invalid_sub_tasks:\n            raise Exception(f\"Sub tasks {invalid_sub_tasks} are not defined.\")\n\n    sub_tasks = sub_tasks or self._selected_sub_tasks or self.sub_tasks\n\n    evaluation_results: list[EvaluationResult] = []\n\n    for sub_task in sub_tasks or [None]:\n        match_groups: list[TaskMatchGroup] = []\n\n        for shots in prompt_shots:\n            if skip_existing_matches and check_task_matches_exists(\n                task_path, shots, sub_task=sub_task\n            ):\n                match_group = TaskMatchGroup.from_file(\n                    task_path, shots, sub_task=sub_task\n                )\n                match_group._loaded_locally = True\n            else:\n                match_group = self.generate_matches(\n                    prompt_lang,\n                    n_shots=shots,\n                    n_first=n_first,\n                    sub_task=sub_task,\n                )\n            match_groups.append(match_group)\n\n        for match_group in match_groups:\n            eval_desc = f\"{match_group.n_shots}-shot\"\n            if sub_task:\n                eval_desc = f\"sub task '{sub_task}' with \" + eval_desc\n            desc = f\"Evaluating {eval_desc} prompt:\"\n            print(desc)\n\n            is_loaded_locally = getattr(match_group, \"_loaded_locally\", False)\n\n            if is_loaded_locally:\n                total_skipped = sum(m.completion is not None for m in match_group)\n                print(\n                    f\"{total_skipped} of {len(match_group)} match completions will be loaded from local.\"\n                )\n\n            try:\n                model.generate_completions(\n                    match_group,\n                    prefer_concurrency=prefer_concurrency,\n                    skip_existing=is_loaded_locally,\n                    n_workers=n_workers,\n                )\n                self.score_matches(match_group)\n            finally:\n                if save_matches:\n                    match_group.save(task_path, sub_task=sub_task)\n\n        evaluation_result = EvaluationResult(\n            model_name=model.model_name,\n            task_name=self.task_name,\n            task_category=self.task_category,\n            score_name=self.score_name,\n            sub_task=sub_task,\n            prompt_shot_results=[\n                PromptShotEvaluationResult(\n                    n_shots=m.n_shots,\n                    score=self.get_overall_score(m),\n                )\n                for m in match_groups\n            ],\n        )\n        evaluation_results.append(evaluation_result)\n\n        if save_evaluation:\n            evaluation_result.save(task_path)\n\n    return evaluation_results\n</code></pre>"},{"location":"tutorial/advanced/","title":"Advanced Tutorial","text":"<p>This section is for the ones who want to implement their own Tasks or want to use the framework APIs for other use cases.</p>"},{"location":"tutorial/advanced/#scores","title":"Scores","text":"<p>Scores are the methods that we use to measure the goodness of the completion of our model, comparing to the expected answer.</p>"},{"location":"tutorial/advanced/#available-scores","title":"Available Scores","text":"Score Name Description Exact Match <code>1</code> if the completion and target are equal, otherwise <code>0</code>. English Sentence Bleu Bleu n-gram score with NLTK English word tokenizer. Between <code>0</code> to <code>1</code>. Persian Sentence Bleu Bleu n-gram score with Hazm Persian word tokenizer. Between <code>0</code> to <code>1</code>. English Rouge Rouge score with NLTK English word tokenizer. Between <code>0</code> to <code>1</code>. Persian Rouge Rouge score with Hazm Persian word tokenizer. Between <code>0</code> to <code>1</code>."},{"location":"tutorial/advanced/#make-your-score","title":"Make Your Score","text":"<p>You can write your score function anywhere and just wrap it using <code>wrap_scorer</code>.</p> <pre><code>import random\nfrom parsbench.scores.base import wrap_scorer\n\n@wrap_scorer\ndef random_score(completion: str, target: str) -&gt; float:\n    return random.random()\n</code></pre> <p>The function's name will be used as the name of your score in the benchmark result.</p>"},{"location":"tutorial/advanced/#data-loader","title":"Data Loader","text":"<p>Data Loader class will load the dataset needed for evaluation.</p>"},{"location":"tutorial/advanced/#jsonline","title":"JSONLine","text":"<p><code>JSONLineDataLoader</code> is based on the <code>jsonlines</code> file format (<code>.jsonl</code>). It can load jsonline files from web or local.</p> <pre><code>from parsbench.tasks.base import JSONLineDataLoader\n\ndata_loader = JSONLineDataLoader(data_path=\"dataset.jsonl\")\ndata = data_loader.load()\n</code></pre>"},{"location":"tutorial/advanced/#csv","title":"CSV","text":"<p><code>CSVDataLoader</code> is based on the <code>CSV</code> file format (<code>.csv</code>). It can load CSV files from web or local.</p> <pre><code>from parsbench.tasks.base import CSVDataLoader\n\ndata_loader = CSVDataLoader(data_path=\"dataset.csv\")\ndata = data_loader.load()\n</code></pre>"},{"location":"tutorial/advanced/#huggingface","title":"HuggingFace","text":"<p><code>HuggingFaceDataLoader</code> is based on the <code>datasets</code> library of HuggingFace that loads datasets from local or downloads it from the HuggingFace Dataset Hub.</p> <pre><code>from parsbench.tasks.base import HuggingFaceDataLoader\n\ndata_loader = HuggingFaceDataLoader(\n    data_path=\"persiannlp/parsinlu_entailment\"\n    split=\"validation\"\n)\ndata = data_loader.load()\n</code></pre>"},{"location":"tutorial/advanced/#prompt-template","title":"Prompt Template","text":"<p>Using <code>PromptTemplate</code> class, you can define the prompt template for different lanugages, shot templates, shot examples, prompt variables, etc.</p>"},{"location":"tutorial/advanced/#with-shot-template","title":"With Shot Template","text":"<p>In this example, you can define a prompt template for sentiment analysis task with shot template.</p> <p>Prompt and Shot Templates:</p> <pre><code>FA_TEMPLATE = \"\"\"\n\u062c\u0645\u0644\u0647 \u0632\u06cc\u0631 \u0646\u0638\u0631 \u06cc\u06a9 \u0634\u062e\u0635 \u0627\u0633\u062a. \u0627\u06cc\u0646 \u062c\u0645\u0644\u0647 \u0628\u0647 \u0632\u0628\u0627\u0646 \u0641\u0627\u0631\u0633\u06cc \u0627\u0633\u062a. \u0628\u0627\u0631 \u06cc\u0627 \u0627\u062d\u0633\u0627\u0633 \u0645\u0648\u062c\u0648\u062f \u062f\u0631 \u0627\u06cc\u0646 \u062c\u0645\u0644\u0647 \u0631\u0627 \u0634\u0646\u0627\u0633\u0627\u06cc\u06cc \u06a9\u0646.\n\u067e\u0627\u0633\u062e\u200c \u0647\u0627\u06cc \u0645\u0645\u06a9\u0646 \u062d\u0627\u0644\u062a\u200c\u0647\u0627\u06cc \u0631\u0648\u0628\u0631\u0648 \u0647\u0633\u062a\u0646\u062f:\nSAD\nNETURAL\nHAPPY\n\n\u0641\u0642\u0637 \u06a9\u0644\u0645\u0647 \u0645\u0631\u0648\u0628\u0637 \u0628\u0647 \u0627\u062d\u0633\u0627\u0633 \u0646\u0638\u0631 \u062f\u0627\u062f\u0647 \u0634\u062f\u0647 \u0631\u0627 \u062e\u0631\u0648\u062c\u06cc \u0628\u062f\u0647.\n\n{example_shots}\n\n\u0646\u0638\u0631: {review}\n\u0627\u062d\u0633\u0627\u0633:\n\"\"\"\n\nFA_SHOT_TEMPLATE = \"\"\"\n\u0646\u0638\u0631: {review}\n\u0627\u062d\u0633\u0627\u0633: {label}\n\"\"\"\n</code></pre> <p>And for the task prompt template:</p> <pre><code>from parsbench.tasks.base import PromptTemplate\n\nprompt_template = PromptTemplate(\n    language_templates={\"fa\": FA_TEMPLATE},\n    prompt_shot_templates={\"fa\": FA_SHOT_TEMPLATE},\n    prompt_variables_mapping={\"review\": \"review\"},\n    target_variables_mapping={\"label\": \"label\"},\n)\n\nprompt = prompt_template.get_prompt(\n    prompt_lang=\"fa\",\n    data={\"review\": \"\u063a\u0630\u0627 \u062e\u06cc\u0644\u06cc \u0628\u062f \u0628\u0648\u062f\", \"label\": \"SAD\"}\n    n_shots=3,\n    sample_data=[\n        {\"review\": \"\u062e\u0648\u0634\u0645\u0632\u0647 \u0628\u0648\u062f \u0645\u0645\u0648\u0646\u0645\", \"label\": \"HAPPY\"},\n        {\"review\": \"\u063a\u0630\u0627 \u062e\u0648\u0628 \u0628\u0648\u062f \u0641\u0642\u0637 \u06a9\u0627\u0634 \u0632\u0648\u062f\u062a\u0631 \u0645\u06cc\u200c\u0631\u0633\u06cc\u062f.\", \"label\": \"NETURAL\"},\n        {\"review\": \"\u0646\u0648\u0634\u0627\u0628\u0647 \u06af\u0631\u0645 \u0628\u0648\u062f. \u067e\u06cc\u062a\u0632\u0627 \u0647\u0645 \u062e\u06cc\u0644\u06cc \u0628\u062f \u0645\u0632\u0647 \u0628\u0648\u062f.\", \"label\": \"SAD\"}\n    ]\n)\n</code></pre>"},{"location":"tutorial/advanced/#with-shot-examples","title":"With Shot Examples","text":"<p>If the task is complicated and you want to use methods like CoT (Chain of Thought) prompting, You can use static shot examples.</p> <pre><code>from parsbench.tasks.base import PromptTemplate\n\nprompt_template = PromptTemplate(\n    language_templates={\"fa\": FA_TEMPLATE},\n    prompt_shot_examples={\"fa\": {1: FA_1_SHOT, 3: FA_3_SHOT, 5: FA_5_SHOT}}\n)\nprompt = prompt_template.get_prompt(prompt_shot=5, ...)\n</code></pre>"},{"location":"tutorial/advanced/#load-templates-from-file","title":"Load Templates From File","text":"<p>You can also load prompts from text files using <code>LazyLoadTemplates</code>.</p> <pre><code>from parsbench.tasks.base import PromptTemplate, LazyLoadTemplates\n\nprompt_template = PromptTemplate(\n    language_templates=LazyLoadTemplates(\n        fa=\"fa_math.txt\",\n        en=\"en_math.txt\",\n    ),\n    ...\n)\n</code></pre>"},{"location":"tutorial/advanced/#constant-prompt-variable","title":"Constant Prompt Variable","text":"<p>Sometimes you may wanna fill some prompt variables with constant data and you don't wanna put it in the prompt template text. You can use <code>ConstantPromptVariable</code>:</p> <pre><code>from parsbench.tasks.base import PromptTemplate, ConstantPromptVariable\n\nprompt_template = PromptTemplate(\n    language_templates={\"fa\": FA_TEMPLATE},\n    prompt_shot_templates={\"fa\": FA_SHOT_TEMPLATE},\n    prompt_variables_mapping={\n        \"input\": \"input\",\n        \"first_name\": ConstantPromptVariable(\"\u0634\u0647\u0631\u06cc\u0627\u0631\")\n    },\n    target_variables_mapping={\"label\": \"label\"},\n)\n</code></pre>"},{"location":"tutorial/advanced/#tasks","title":"Tasks","text":"<p>The primary unit of the ParsBench framework is a task. Tasks are battery-included evaluators which do all the process from loading dataset to evaluating models and outputting the result.</p>"},{"location":"tutorial/advanced/#task-data-provider","title":"Task Data Provider","text":"<p>Each task includes a dataset for evaluation. If you need to get the data of that task, you can use <code>task.get_data</code> function.</p> <pre><code>from parsbench.tasks import ParsiNLUEntailment\n\nwith ParsiNLUEntailment() as task:  # Open in context manager to load data.\n    data = task.get_data()\n</code></pre>"},{"location":"tutorial/advanced/#task-match-generator","title":"Task Match Generator","text":"<p>A <code>TaskMatch</code> is an object which includes prompt, target answer, model completion and the score. Initially the matches doesn't have <code>completion</code> and <code>score</code> attributes (default to <code>None</code>). But you can generate completions and score them using the task class itself.</p> <p>To generate matches based on the prompt template, you can use following code:</p> <pre><code>from parsbench.tasks import ParsiNLUEntailment\n\nwith ParsiNLUEntailment() as task:\n    matches = task.generate_matches(prompt_lang=\"fa\", n_shots=0, n_first=100)\n</code></pre>"},{"location":"tutorial/advanced/#generate-completions-and-score","title":"Generate Completions and Score","text":"<p>After generating matches, now we should generate completions for each match prompt. Then we score them based on the defined scorer in the task.</p> <pre><code>from parsbench.tasks import ParsiNLUEntailment\n\nwith ParsiNLUEntailment() as task:\n    matches = task.generate_matches(prompt_lang=\"fa\", n_shots=0, n_first=100)\n    model.generate_completions(matches)  # Completions are generated by the model.\n    tasks.score_matches(matches)\n</code></pre>"},{"location":"tutorial/advanced/#make-your-task","title":"Make Your Task","text":"<p>If you want to have a task with your own private dataset, prompts, setups, etc. You can inherit one of the existing tasks or create your own task from scratch.</p> <p>We suggest to put your prompt templates in a text file and use <code>LazyLoadTemplates</code> to load them for a better performance.</p> <pre><code>from parsbench.scores.base import Scorer, wrap_scorer\nfrom parsbench.tasks.base import (\n    HuggingFaceDataLoader,\n    LazyLoadTemplates,\n    PromptTemplate,\n    Task,\n    TaskCategory,\n    TaskMatchGroup,\n)\n\n@wrap_scorer\ndef my_custom_score(completion: str, target: str) -&gt; float:\n    return float(completion.strip() == target.strip())\n\nclass CustomTask(Task):\n    task_name: str = \"Custom Task\"\n    task_category: TaskCategory = TaskCategory.REASONING\n\n    data_loader: HuggingFaceDataLoader = HuggingFaceDataLoader(\n        data_path=\"org/custom_dataset\",\n        split=\"test\",\n    )\n    data_target_key: str = \"target\"\n\n    prompt_template: PromptTemplate = PromptTemplate(\n        language_templates=LazyLoadTemplates(\n            en=\"path/to/en_template.txt\",\n            fa=\"path/to/fa_template.txt\",\n        ),\n        prompt_shot_templates=LazyLoadTemplates(\n            en=\"path/to/en_shot_template.txt\",\n            fa=\"path/to/fa_shot_template.txt\",\n        ),\n        prompt_variables_mapping={\"prompt_variable1\": \"variable1\", \"prompt_variable2\": \"variable2\"},\n        target_variables_mapping={\"prompt_target\": \"target\"},\n    )\n\n    scorer: Scorer = my_custom_score\n\n    def score_matches(self, matches: TaskMatchGroup) -&gt; TaskMatchGroup:\n        matches.format_completions(\n            lambda c: c.strip().strip(\"'\").lower()\n        )\n        return super().score_matches(matches)\n\n    def get_overall_score(cls, matches: TaskMatchGroup) -&gt; float:\n        return sum(match.score for match in matches) / len(matches)\n</code></pre> <p>You can use any data loader, prompt template and scorer you want.</p>"},{"location":"tutorial/advanced/#add-sub-tasks","title":"Add Sub Tasks","text":"<p>Your task might have a couple of sub tasks. In that case you should specify <code>sub_task_key</code> and <code>sub_tasks</code> attributes in the task class.</p> <pre><code>class CustomTask(Task):\n    ...\n    sub_task_key: str = \"category\"  # Column in the dataset which specify the sub tasks.\n    sub_tasks: list[str] = [\"math_and_logic\", \"common_knowledge\", \"literature\"]  # Expected sub tasks.\n    ...\n</code></pre> <p>And for generating matches or evaluating the model, you can specify a subset of the sub tasks.</p> <pre><code>with CustomTask() as task:\n    results = task.evaluate(..., sub_tasks=[\"math_and_logic\"])\n\n# OR\n\nbenchmark = CustomBenchmark(\n    ...,\n    tasks=[\n        PersianMath,\n        CustomTask.select_sub_tasks([\"math_and_logic\"]),\n    ]\n)\n</code></pre>"},{"location":"tutorial/benchmarks/","title":"Benchmarks","text":"<p>Using Benchmarks you can evaluate different Models based on different Tasks and compare their score.</p>"},{"location":"tutorial/benchmarks/#custom-benchmark","title":"Custom Benchmark","text":"<p>You can easily create a benchmark with your desired tasks and models.</p> <pre><code>from transformers import AutoModelForCausalLM, AutoTokenizer\n\nfrom parsbench.benchmarks import CustomBenchmark\nfrom parsbench.models import OpenAIModel, PreTrainedTransformerModel\nfrom parsbench.tasks import ParsiNLUMultipleChoice, PersianMath, ParsiNLUReadingComprehension\n\n# Create Models\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen2-72B-Instruct\",\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-72B-Instruct\")\nqwen2_model = PreTrainedTransformerModel(model=model, tokenizer=tokenizer)\n\naya_model = OpenAIModel(\n    api_base_url=\"http://localhost:11434/v1/\",\n    api_secret_key=\"ollama\",\n    model=\"aya:latest\",\n)\n\n# Run Benchmark\nbenchmark = CustomBenchmark(\n    models=[qwen2_model, aya_model],\n    tasks=[\n        ParsiNLUMultipleChoice,\n        ParsiNLUReadingComprehension,\n        PersianMath,\n    ],\n)\nresult = benchmark.run(\n    prompt_lang=\"fa\",\n    prompt_shots=[0, 3],\n    n_first=100,\n    sort_by_score=True,\n)\n</code></pre>"},{"location":"tutorial/benchmarks/#full-benchmark","title":"Full Benchmark","text":"<p>To benchmark your model based on all existing tasks in the framework. You can use <code>load_all_tasks</code> function.</p> <pre><code>from parsbench.benchmarks import CustomBenchmark\nfrom parsbench.models import OpenAIModel\nfrom parsbench.tasks.utils import load_all_tasks\n\naya_model = OpenAIModel(\n    api_base_url=\"http://localhost:11434/v1/\",\n    api_secret_key=\"ollama\",\n    model=\"aya:latest\",\n)\n\n# Run Benchmark\nbenchmark = CustomBenchmark(\n    models=[aya_model],\n    tasks=load_all_tasks(),\n)\nresult = benchmark.run(\n    prompt_lang=\"fa\",\n    prompt_shots=[0, 3],\n    n_first=100,\n    sort_by_score=True,\n)\n</code></pre>"},{"location":"tutorial/benchmarks/#benchmark-result","title":"Benchmark Result","text":"<p>The benchmark result contains all evaluation results for each model. You can use it directly or convert it to a Pandas DataFrame with <code>to_pandas</code> function. If you want to get a pivot table of benchmark result, you should use <code>to_pandas(pivot=True)</code>.</p> <pre><code>print(result.to_pandas(pivot=True))\n</code></pre> <p>Output should be like:</p> <pre><code>                                                                                     score          \nmodel_name                                                                     qwen2:latest          \nn_shots                                                                                   0         3\ntask_category task_name                      sub_task         score_name                             \nclassic       ParsiNLU Reading Comprehension NaN              Common Tokens         0.46231  0.588274\nknowledge     PersiNLU Multiple Choice       common_knowledge Exact Match           0.30000  0.000000\n                                             literature       Exact Match           0.20000  0.428571\n                                             math_and_logic   Exact Match           0.60000  0.285714\nmath          Persian Math                   NaN              Math Equivalence      0.00000  0.142857\n</code></pre> <p>Note: It would look better if you run it in a Jupyter Notebook.</p>"},{"location":"tutorial/benchmarks/#radar-plot-spider-plot","title":"Radar Plot (Spider Plot)","text":"<p>For a better comparison between models performance on different tasks. You can use <code>show_radar_plot</code> to visualize the benchmark.</p> <pre><code>result.show_radar_plot()\n</code></pre> <p>Output should be like:</p> <p></p>"},{"location":"tutorial/benchmarks/#save-result","title":"Save Result","text":"<p>To save the matches, evaluations and benchmark results during the benchmarking process, you can set <code>save_matches</code>, <code>save_evaluation</code> and <code>save_benchmark</code>.</p> <pre><code>benchmark = CustomBenchmark(\n    models=[aya_model, qwen2_model],\n    tasks=[PersianMath, FarsTailEntailment],\n)\nresult = benchmark.run(\n    prompt_lang=\"fa\",\n    prompt_shots=[0, 5],\n    n_first=100,\n    save_matches=True,\n    save_evaluation=True,\n    save_benchmark=True,\n    output_path=\"results\",\n    sort_by_score=True,\n)\n</code></pre> <p>The output directory structure should be like this:</p> <pre><code>results\n\u251c\u2500\u2500 aya:latest\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 FarsTail_Entailment\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 evaluation.jsonl\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 matches_0_shot.jsonl\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 matches_5_shot.jsonl\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 Persian_Math\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 evaluation.jsonl\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 matches_0_shot.jsonl\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 matches_5_shot.jsonl\n\u251c\u2500\u2500 qwen2:latest\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 FarsTail_Entailment\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 evaluation.jsonl\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 matches_0_shot.jsonl\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 matches_5_shot.jsonl\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 Persian_Math\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 evaluation.jsonl\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 matches_0_shot.jsonl\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 matches_5_shot.jsonl\n\u2514\u2500\u2500 benchmark.jsonl\n</code></pre>"},{"location":"tutorial/models/","title":"Models","text":"<p>Models are the interfaces that enable you to generate completions using LLMs.</p>"},{"location":"tutorial/models/#openaimodel","title":"OpenAIModel","text":"<p>The <code>OpenAIModel</code> is an interface for using OpenAI-like APIs to generate completions.</p> <pre><code>from parsbench.models import OpenAIModel\n\nmodel = OpenAIModel(\n    api_base_url=\"https://api.openai.com/v1/\",\n    api_secret_key=\"{SECRET_KEY}\",\n    model=\"gpt-4o\",\n)\n</code></pre> <p>Use can run your local model using for example <code>Ollama</code>:</p> <pre><code>ollama run llama3\n</code></pre> <p>And use its API:</p> <pre><code>from parsbench.models import OpenAIModel\n\nmodel = OpenAIModel(\n    api_base_url=\"http://localhost:11434/v1/\",\n    api_secret_key=\"ollama\",\n    model=\"llama3:latest\",\n)\n</code></pre>"},{"location":"tutorial/models/#anthropicmodel","title":"AnthropicModel","text":"<p>The <code>AnthropicModel</code> is an interface for using Anthropic-like APIs to generate completions.</p> <pre><code>from parsbench.models import AnthropicModel\n\nmodel = AnthropicModel(\n    api_secret_key=\"{SECRET_KEY}\",\n    model=\"claude3.5-sonnet\",\n)\n</code></pre>"},{"location":"tutorial/models/#pretrainedtransformermodel","title":"PreTrainedTransformerModel","text":"<p>The <code>PreTrainedTransformerModel</code> is an interface for the <code>PreTrainedModel</code> of the transformers framework. You can load any pre-trained model and tokenizer you want and pass it to the <code>PreTrainedTransformerModel</code>. And it will generate completions using your own model.</p> <pre><code>from transformers import AutoModelForCausalLM, AutoTokenizer\n\nfrom parsbench.models import PreTrainedTransformerModel\nfrom parsbench.tasks import PersianMath\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen2-72B-Instruct\",\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-72B-Instruct\")\n\ntf_model = PreTrainedTransformerModel(model=model, tokenizer=tokenizer)\n\nwith PersianMath() as task:\n    results = task.evaluate(tf_model)\n</code></pre>"},{"location":"tutorial/models/#create-your-own-interface","title":"Create Your Own Interface","text":"<p>You can easily create your own model interface by inheriting the <code>Model</code> abstract class:</p> <pre><code>from parsbench.models import Model\n\nclass CustomModel(Model):\n    @property\n    def model_name(self) -&gt; str:\n        return \"My Custom Model\"\n\n    def get_prompt_completion(self, prompt: str) -&gt; str:\n        return f\"Response to {prompt}\"\n\n    def prompt_formater(self, prompt: str) -&gt; str | list[dict]:\n        return prompt  # No format\n\n    def completion_formatter(self, completion: str) -&gt; str:\n        return completion.strip().replace(\"'\", \"\")\n</code></pre>"},{"location":"tutorial/tasks/","title":"Tasks","text":"<p>Tasks are used to test and evaluate the Model responses. They come with a dataset of questions and expected answers. The task will generate prompts based on the prompt template and data, get the completion of that prompt using the Model, and then score it using the specified score.</p>"},{"location":"tutorial/tasks/#available-tasks","title":"Available Tasks","text":"Task Name Score Name Dataset ParsiNLU Sentiment Analysis Exact Match (F1) ParsiNLU ParsiNLU Entailment Exact Match (F1) ParsiNLU ParsiNLU Machine Translation En -&gt; Fa Bleu ParsiNLU ParsiNLU Machine Translation Fa -&gt; En Bleu ParsiNLU ParsiNLU Multiple Choice Exact Match (Accuracy) ParsiNLU ParsiNLU Reading Comprehension Common Tokens (F1) ParsiNLU Persian NER NER Exact Match (F1) PersianNER Persian Math Math Equivalence (Accuracy) Source ConjNLI Entailment Exact Match (F1) Source Persian MMLU (Khayyam Challenge) Exact Match (Accuracy) Khayyam Challenge FarsTail Entailment Exact Match (F1) FarsTail Persian News Summary Rouge PNSummary XL-Sum Rouge XLSum <p>You can import the class of above tasks from <code>parsbench.tasks</code> and use it for evaluating your model.</p>"},{"location":"tutorial/tasks/#evaluation","title":"Evaluation","text":"<p>The evaluation process has 6 steps:</p> <ol> <li>Loading Data</li> <li>Loading Prompt Template</li> <li>Generating Matches (Prompt-Answer)</li> <li>Generating Completions</li> <li>Scoring Completions</li> <li>Storing Result (Optional)</li> </ol> <p>Here is an example of evaluating a pre-trained model on PersianMath:</p> <pre><code>from transformers import AutoModelForCausalLM, AutoTokenizer\n\nfrom parsbench.models import PreTrainedTransformerModel\nfrom parsbench.tasks import ParsiNLUMultipleChoice\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen2-72B-Instruct\",\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-72B-Instruct\")\n\ntf_model = PreTrainedTransformerModel(model=model, tokenizer=tokenizer)\n\nwith ParsiNLUMultipleChoice() as task:\n    results = task.evaluate(\n        model=tf_model,\n        prompt_lang=\"fa\",\n        prompt_shots=[0, 5],\n    )\n</code></pre> <p>You should use the task in a context manager. It manages data loading and offloading for a better performance.</p>"},{"location":"tutorial/tasks/#evaluation-result","title":"Evaluation Result","text":"<p>The <code>evaluate</code> function of a task will return a list of <code>EvaluationResult</code> data classe which contains the overall score for each sub task and n_shot prompt of the task.</p> <p>You can directly use the class or convert it to a Pandas DataFrame with <code>to_pandas</code> function.</p> <pre><code>eval_result = results[0]\nprint(eval_result.to_pandas())\n</code></pre> <p>Output:</p> <pre><code>     model_name                 task_name task_category        sub_task  n_shots   score_name     score\n0  qwen2:latest  PersiNLU Multiple Choice     knowladge  math_and_logic        0  Exact Match  0.600000\n1  qwen2:latest  PersiNLU Multiple Choice     knowladge  math_and_logic        3  Exact Match  0.285714\n</code></pre>"},{"location":"tutorial/tasks/#save-result","title":"Save Result","text":"<p>You can manually save the result using <code>save</code> function of the <code>EvaluationResult</code> or pass <code>save_evaluation=True</code> to the <code>evaluate</code> function.</p> <p>You can also save task matches which contains prompt, completion, target, and score by passing <code>save_matches=True</code> to the <code>evaluate</code> function.</p> <pre><code>with PersianMath() as task:\n    results = task.evaluate(\n        model=tf_model,\n        prompt_lang=\"fa\",\n        prompt_shots=[0, 5],\n        save_matches=True,\n        save_evaluation=True,\n        output_path=\"results/\",\n    )\n</code></pre> <p>The output directory structure should be like this:</p> <pre><code>results\n\u2514\u2500\u2500 qwen2:latest\n    \u2514\u2500\u2500 Persian_Math\n        \u251c\u2500\u2500 evaluation.jsonl\n        \u251c\u2500\u2500 matches_0_shot.jsonl\n        \u2514\u2500\u2500 matches_5_shot.jsonl\n</code></pre>"}]}